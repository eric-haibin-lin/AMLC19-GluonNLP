{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Deployment with BERT\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Pre-trained language representations have been shown to improve many downstream NLP tasks such as question answering, and natural language inference. Devlin, Jacob, et al proposed BERT [1] (Bidirectional Encoder Representations from Transformers), which fine-tunes deep bidirectional representations on a wide range of tasks with minimal task-specific parameters, and obtained state- of-the-art results.\n",
    "\n",
    "In this tutorial, we will focus on adapting the BERT model for the question answering task on the SQuAD dataset. Specifically, we will:\n",
    "\n",
    "- understand how to pre-process the SQuAD dataset to leverage the learnt representation in BERT,\n",
    "- adapt the BERT model to the question answering task, and\n",
    "- load a trained model to perform inference on the SQuAD dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Load MXNet and GluonNLP\n",
    "\n",
    "We first import the libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-14T01:45:27.416134Z",
     "start_time": "2019-06-14T01:45:26.339759Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import collections, time, logging\n",
    "import numpy as np\n",
    "import gluonnlp as nlp\n",
    "import mxnet as mx\n",
    "import bert\n",
    "import qa_utils\n",
    "from bert.bert_qa_evaluate import PredResult, predict\n",
    "import tvm\n",
    "from tvm import relay\n",
    "import topi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Inspect the SQuAD Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Then we take a look at the Stanford Question Answering Dataset (SQuAD). The dataset can be downloaded using the `nlp.data.SQuAD` API. In this tutorial, we create a small dataset with 3 samples from the SQuAD dataset for demonstration purpose.\n",
    "\n",
    "The question answering task on the SQuAD dataset is setup the following way. For each sample in the dataset, a context is provided. The context is usually a long paragraph which contains lots of information. Then a question asked based on the context. The goal is to find the text span in the context that answers the question in the sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-14T01:45:27.555133Z",
     "start_time": "2019-06-14T01:45:27.418706Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples in the created dataset subsampled from SQuAD = 3\n"
     ]
    }
   ],
   "source": [
    "full_data = nlp.data.SQuAD(segment='dev', version='1.1')\n",
    "# loading a subset of the dev set of SQuAD\n",
    "num_target_samples = 3\n",
    "target_samples = [full_data[i] for i in range(num_target_samples)]\n",
    "dataset = mx.gluon.data.SimpleDataset(target_samples)\n",
    "print('Number of samples in the created dataset subsampled from SQuAD = %d'%len(dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Let's take a look at a sample from the dataset. In this sample, the question is about the location of the game, with a description about the Super Bowl 50 game as the context. Note that three different answer spans are correct for this question, and they start from index 403, 355 and 355 in the context respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-14T01:45:27.560564Z",
     "start_time": "2019-06-14T01:45:27.557274Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Context:\n",
      "Super Bowl 50 was an American football game to determine the champion of the National Football League (NFL) for the 2015 season. The American Football Conference (AFC) champion Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers 24–10 to earn their third Super Bowl title. The game was played on February 7, 2016, at Levi's Stadium in the San Francisco Bay Area at Santa Clara, California. As this was the 50th Super Bowl, the league emphasized the \"golden anniversary\" with various gold-themed initiatives, as well as temporarily suspending the tradition of naming each Super Bowl game with Roman numerals (under which the game would have been known as \"Super Bowl L\"), so that the logo could prominently feature the Arabic numerals 50.\n"
     ]
    }
   ],
   "source": [
    "sample = dataset[2]\n",
    "\n",
    "context_idx = 3\n",
    "\n",
    "print('\\nContext:')\n",
    "print(sample[context_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-14T01:45:27.567303Z",
     "start_time": "2019-06-14T01:45:27.562425Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Question\n",
      "Where did Super Bowl 50 take place?\n",
      "\n",
      "Correct Answer Spans\n",
      "['Santa Clara, California', \"Levi's Stadium\", \"Levi's Stadium in the San Francisco Bay Area at Santa Clara, California.\"]\n",
      "\n",
      "Answer Span Start Indices:\n",
      "[403, 355, 355]\n"
     ]
    }
   ],
   "source": [
    "question_idx = 2\n",
    "answer_idx = 4\n",
    "answer_pos_idx = 5\n",
    "\n",
    "print(\"\\nQuestion\")\n",
    "print(sample[question_idx])\n",
    "print(\"\\nCorrect Answer Spans\")\n",
    "print(sample[answer_idx])\n",
    "print(\"\\nAnswer Span Start Indices:\")\n",
    "print(sample[answer_pos_idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Data Pre-processing for QA with BERT\n",
    "\n",
    "Recall that during BERT pre-training, it takes a sentence pair as the input, separated by the 'SEP' special token. For SQuAD, we can feed the context-question pair as the sentence pair input. To use BERT to predict the starting and ending span of the answer, we can add a classification layer for each token in the context texts, to predict if a token is the start or the end of the answer span. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "In the next few code blocks, we will work on pre-processing the samples in the SQuAD dataset in the desired format with these special separators. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "First, let's use the *get_model* API in GluonNLP to get the model definition for BERT, and the vocabulary used for the BERT model. Note that we discard the pooler and classifier layers used for the next sentence prediction task, as well as the decoder layers for the masked language model task during the BERT pre-training phase. These layers are not useful for predicting the starting and ending indices of the answer span."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-14T01:45:27.715444Z",
     "start_time": "2019-06-14T01:45:27.569118Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from bert.export.hybrid_bert import HybridBERTForQA, get_hybrid_model\n",
    "seq_length = 256\n",
    "bert, vocab = get_hybrid_model(\n",
    "    name=\"bert_12_768_12\",\n",
    "    dataset_name=\"book_corpus_wiki_en_uncased\",\n",
    "    pretrained=False,\n",
    "    use_pooler=False,\n",
    "    use_decoder=False,\n",
    "    use_classifier=False,\n",
    "    seq_length=seq_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Note that there are several special tokens in the vocabulary for BERT. In particular, the `[SEP]` token is used for separating the sentence pairs, and the `[CLS]` token is added at the beginning of the sentence pairs. They will be used to pre-process the SQuAD dataset later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-14T01:45:27.720137Z",
     "start_time": "2019-06-14T01:45:27.717192Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab(size=30522, unk=\"[UNK]\", reserved=\"['[CLS]', '[SEP]', '[MASK]', '[PAD]']\")\n"
     ]
    }
   ],
   "source": [
    "print(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "The second step is to process the samples using the same tokenizer used for BERT, which is provided as the `BERTTokenizer` API in GluonNLP. Note that instead of word level and character level representation, BERT uses subwords to represent a word, separated `##`. \n",
    "\n",
    "In the following example, the word `suspending` is tokenized as two subwords (`suspend` and `##ing`), and `numerals` is tokenized as three subwords (`nu`, `##meral`, `##s`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-14T01:45:27.731724Z",
     "start_time": "2019-06-14T01:45:27.721690Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['as',\n",
       " 'well',\n",
       " 'as',\n",
       " 'temporarily',\n",
       " 'suspend',\n",
       " '##ing',\n",
       " 'the',\n",
       " 'tradition',\n",
       " 'of',\n",
       " 'naming',\n",
       " 'each',\n",
       " 'super',\n",
       " 'bowl',\n",
       " 'game',\n",
       " 'with',\n",
       " 'roman',\n",
       " 'nu',\n",
       " '##meral',\n",
       " '##s']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = nlp.data.BERTTokenizer(vocab=vocab, lower=True)\n",
    "\n",
    "tokenizer(\"as well as temporarily suspending the tradition of naming each Super Bowl game with Roman numerals\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Sentence Pair Composition\n",
    "\n",
    "With the tokenizer inplace, we are ready to process the question-context texts and compose sentence pairs. The functionality is available via the `SQuADTransform` API. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-14T01:45:27.897684Z",
     "start_time": "2019-06-14T01:45:27.734029Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done! Transform dataset costs 0.10 seconds.\n"
     ]
    }
   ],
   "source": [
    "transform = bert.data.qa.SQuADTransform(tokenizer, is_pad=False, is_training=False, do_lookup=False)\n",
    "dev_data_transform, _ = bert.data.qa.preprocess_dataset(dataset, transform)\n",
    "logging.info('The number of examples after preprocessing:{}'.format(len(dev_data_transform)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let's take a look at the sample after the transformation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-14T01:45:27.904353Z",
     "start_time": "2019-06-14T01:45:27.899992Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "segment type: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "\n",
      "text length: 168\n",
      "\n",
      "sentence pair: ['[CLS]', 'where', 'did', 'super', 'bowl', '50', 'take', 'place', '?', '[SEP]', 'super', 'bowl', '50', 'was', 'an', 'american', 'football', 'game', 'to', 'determine', 'the', 'champion', 'of', 'the', 'national', 'football', 'league', '(', 'nfl', ')', 'for', 'the', '2015', 'season', '.', 'the', 'american', 'football', 'conference', '(', 'afc', ')', 'champion', 'denver', 'broncos', 'defeated', 'the', 'national', 'football', 'conference', '(', 'nfc', ')', 'champion', 'carolina', 'panthers', '24', '–', '10', 'to', 'earn', 'their', 'third', 'super', 'bowl', 'title', '.', 'the', 'game', 'was', 'played', 'on', 'february', '7', ',', '2016', ',', 'at', 'levi', \"'\", 's', 'stadium', 'in', 'the', 'san', 'francisco', 'bay', 'area', 'at', 'santa', 'clara', ',', 'california', '.', 'as', 'this', 'was', 'the', '50th', 'super', 'bowl', ',', 'the', 'league', 'emphasized', 'the', '\"', 'golden', 'anniversary', '\"', 'with', 'various', 'gold', '-', 'themed', 'initiatives', ',', 'as', 'well', 'as', 'temporarily', 'suspend', '##ing', 'the', 'tradition', 'of', 'naming', 'each', 'super', 'bowl', 'game', 'with', 'roman', 'nu', '##meral', '##s', '(', 'under', 'which', 'the', 'game', 'would', 'have', 'been', 'known', 'as', '\"', 'super', 'bowl', 'l', '\"', ')', ',', 'so', 'that', 'the', 'logo', 'could', 'prominently', 'feature', 'the', 'arabic', 'nu', '##meral', '##s', '50', '.', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "sample = dev_data_transform[2]\n",
    "print('\\nsegment type: ' + str(sample[2]))\n",
    "print('\\ntext length: ' + str(sample[3]))\n",
    "print('\\nsentence pair: ' + str(sample[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Vocabulary Lookup\n",
    "\n",
    "Finally, we convert the transformed texts to subword indices, which are used to contructor NDArrays as the inputs to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-14T01:45:27.910853Z",
     "start_time": "2019-06-14T01:45:27.906127Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 2073, 2106, 3565, 4605, 2753, 2202, 2173, 1029, 3, 3565, 4605, 2753, 2001, 2019, 2137, 2374, 2208, 2000, 5646, 1996, 3410, 1997, 1996, 2120, 2374, 2223, 1006, 5088, 1007, 2005, 1996, 2325, 2161, 1012, 1996, 2137, 2374, 3034, 1006, 10511, 1007, 3410, 7573, 14169, 3249, 1996, 2120, 2374, 3034, 1006, 22309, 1007, 3410, 3792, 12915, 2484, 1516, 2184, 2000, 7796, 2037, 2353, 3565, 4605, 2516, 1012, 1996, 2208, 2001, 2209, 2006, 2337, 1021, 1010, 2355, 1010, 2012, 11902, 1005, 1055, 3346, 1999, 1996, 2624, 3799, 3016, 2181, 2012, 4203, 10254, 1010, 2662, 1012, 2004, 2023, 2001, 1996, 12951, 3565, 4605, 1010, 1996, 2223, 13155, 1996, 1000, 3585, 5315, 1000, 2007, 2536, 2751, 1011, 11773, 11107, 1010, 2004, 2092, 2004, 8184, 28324, 2075, 1996, 4535, 1997, 10324, 2169, 3565, 4605, 2208, 2007, 3142, 16371, 28990, 2015, 1006, 2104, 2029, 1996, 2208, 2052, 2031, 2042, 2124, 2004, 1000, 3565, 4605, 1048, 1000, 1007, 1010, 2061, 2008, 1996, 8154, 2071, 14500, 3444, 1996, 5640, 16371, 28990, 2015, 2753, 1012, 3]\n"
     ]
    }
   ],
   "source": [
    "def vocab_lookup(example_id, subwords, type_ids, length, start, end):\n",
    "    indices = vocab[subwords]\n",
    "    return example_id, indices, type_ids, length, start, end\n",
    "\n",
    "dev_data_transform = dev_data_transform.transform(vocab_lookup, lazy=False)\n",
    "print(dev_data_transform[2][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We download a BERT model trained on the SQuAD dataset, prepare the dataloader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-14T01:45:32.221383Z",
     "start_time": "2019-06-14T01:45:27.922825Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded checkpoint to ./temp/bert_qa-7eb11865.params\n"
     ]
    }
   ],
   "source": [
    "net = HybridBERTForQA(bert)\n",
    "ctx = mx.cpu()\n",
    "ckpt = qa_utils.download_qa_ckpt()\n",
    "net.load_parameters(ckpt, ctx=ctx)\n",
    "\n",
    "# batch_size = 1\n",
    "# dev_dataloader = mx.gluon.data.DataLoader(\n",
    "#     dev_data_transform, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v0.0.1\n",
      "def @main(%data2: Tensor[(1,), float32], %hybridbertmodel2_word_embed_embedding0_weight: Tensor[(30522, 768), float32], %data0: Tensor[(1, 256), float32], %hybridbertmodel2_token_type_embed_embedding0_weight: Tensor[(2, 768), float32], %data1: Tensor[(1, 256), float32], %hybridbertencoder2_position_weight: Tensor[(512, 768), float32], %hybridbertencoder2_bertlayernorm0_gamma: Tensor[(768,), float32], %hybridbertencoder2_bertlayernorm0_beta: Tensor[(768,), float32], %hybridbertencoder2_transformer0_multiheadattentioncell0_query_weight: Tensor[(768, 768), float32], %hybridbertencoder2_transformer0_multiheadattentioncell0_query_bias: Tensor[(768,), float32], %hybridbertencoder2_transformer0_multiheadattentioncell0_key_weight: Tensor[(768, 768), float32], %hybridbertencoder2_transformer0_multiheadattentioncell0_key_bias: Tensor[(768,), float32], %hybridbertencoder2_transformer0_multiheadattentioncell0_value_weight: Tensor[(768, 768), float32], %hybridbertencoder2_transformer0_multiheadattentioncell0_value_bias: Tensor[(768,), float32], %hybridbertencoder2_transformer0_proj_weight: Tensor[(768, 768), float32], %hybridbertencoder2_transformer0_proj_bias: Tensor[(768,), float32], %hybridbertencoder2_transformer0_bertlayernorm0_gamma: Tensor[(768,), float32], %hybridbertencoder2_transformer0_bertlayernorm0_beta: Tensor[(768,), float32], %hybridbertencoder2_transformer0_bertpositionwiseffn0_ffn_1_weight: Tensor[(3072, 768), float32], %hybridbertencoder2_transformer0_bertpositionwiseffn0_ffn_1_bias: Tensor[(3072,), float32], %hybridbertencoder2_transformer0_bertpositionwiseffn0_ffn_2_weight: Tensor[(768, 3072), float32], %hybridbertencoder2_transformer0_bertpositionwiseffn0_ffn_2_bias: Tensor[(768,), float32], %hybridbertencoder2_transformer0_bertpositionwiseffn0_bertlayernorm0_gamma: Tensor[(768,), float32], %hybridbertencoder2_transformer0_bertpositionwiseffn0_bertlayernorm0_beta: Tensor[(768,), float32], %hybridbertencoder2_transformer1_multiheadattentioncell0_query_weight: Tensor[(768, 768), float32], %hybridbertencoder2_transformer1_multiheadattentioncell0_query_bias: Tensor[(768,), float32], %hybridbertencoder2_transformer1_multiheadattentioncell0_key_weight: Tensor[(768, 768), float32], %hybridbertencoder2_transformer1_multiheadattentioncell0_key_bias: Tensor[(768,), float32], %hybridbertencoder2_transformer1_multiheadattentioncell0_value_weight: Tensor[(768, 768), float32], %hybridbertencoder2_transformer1_multiheadattentioncell0_value_bias: Tensor[(768,), float32], %hybridbertencoder2_transformer1_proj_weight: Tensor[(768, 768), float32], %hybridbertencoder2_transformer1_proj_bias: Tensor[(768,), float32], %hybridbertencoder2_transformer1_bertlayernorm0_gamma: Tensor[(768,), float32], %hybridbertencoder2_transformer1_bertlayernorm0_beta: Tensor[(768,), float32], %hybridbertencoder2_transformer1_bertpositionwiseffn0_ffn_1_weight: Tensor[(3072, 768), float32], %hybridbertencoder2_transformer1_bertpositionwiseffn0_ffn_1_bias: Tensor[(3072,), float32], %hybridbertencoder2_transformer1_bertpositionwiseffn0_ffn_2_weight: Tensor[(768, 3072), float32], %hybridbertencoder2_transformer1_bertpositionwiseffn0_ffn_2_bias: Tensor[(768,), float32], %hybridbertencoder2_transformer1_bertpositionwiseffn0_bertlayernorm0_gamma: Tensor[(768,), float32], %hybridbertencoder2_transformer1_bertpositionwiseffn0_bertlayernorm0_beta: Tensor[(768,), float32], %hybridbertencoder2_transformer2_multiheadattentioncell0_query_weight: Tensor[(768, 768), float32], %hybridbertencoder2_transformer2_multiheadattentioncell0_query_bias: Tensor[(768,), float32], %hybridbertencoder2_transformer2_multiheadattentioncell0_key_weight: Tensor[(768, 768), float32], %hybridbertencoder2_transformer2_multiheadattentioncell0_key_bias: Tensor[(768,), float32], %hybridbertencoder2_transformer2_multiheadattentioncell0_value_weight: Tensor[(768, 768), float32], %hybridbertencoder2_transformer2_multiheadattentioncell0_value_bias: Tensor[(768,), float32], %hybridbertencoder2_transformer2_proj_weight: Tensor[(768, 768), float32], %hybridbertencoder2_transformer2_proj_bias: Tensor[(768,), float32], %hybridbertencoder2_transformer2_bertlayernorm0_gamma: Tensor[(768,), float32], %hybridbertencoder2_transformer2_bertlayernorm0_beta: Tensor[(768,), float32], %hybridbertencoder2_transformer2_bertpositionwiseffn0_ffn_1_weight: Tensor[(3072, 768), float32], %hybridbertencoder2_transformer2_bertpositionwiseffn0_ffn_1_bias: Tensor[(3072,), float32], %hybridbertencoder2_transformer2_bertpositionwiseffn0_ffn_2_weight: Tensor[(768, 3072), float32], %hybridbertencoder2_transformer2_bertpositionwiseffn0_ffn_2_bias: Tensor[(768,), float32], %hybridbertencoder2_transformer2_bertpositionwiseffn0_bertlayernorm0_gamma: Tensor[(768,), float32], %hybridbertencoder2_transformer2_bertpositionwiseffn0_bertlayernorm0_beta: Tensor[(768,), float32], %hybridbertencoder2_transformer3_multiheadattentioncell0_query_weight: Tensor[(768, 768), float32], %hybridbertencoder2_transformer3_multiheadattentioncell0_query_bias: Tensor[(768,), float32], %hybridbertencoder2_transformer3_multiheadattentioncell0_key_weight: Tensor[(768, 768), float32], %hybridbertencoder2_transformer3_multiheadattentioncell0_key_bias: Tensor[(768,), float32], %hybridbertencoder2_transformer3_multiheadattentioncell0_value_weight: Tensor[(768, 768), float32], %hybridbertencoder2_transformer3_multiheadattentioncell0_value_bias: Tensor[(768,), float32], %hybridbertencoder2_transformer3_proj_weight: Tensor[(768, 768), float32], %hybridbertencoder2_transformer3_proj_bias: Tensor[(768,), float32], %hybridbertencoder2_transformer3_bertlayernorm0_gamma: Tensor[(768,), float32], %hybridbertencoder2_transformer3_bertlayernorm0_beta: Tensor[(768,), float32], %hybridbertencoder2_transformer3_bertpositionwiseffn0_ffn_1_weight: Tensor[(3072, 768), float32], %hybridbertencoder2_transformer3_bertpositionwiseffn0_ffn_1_bias: Tensor[(3072,), float32], %hybridbertencoder2_transformer3_bertpositionwiseffn0_ffn_2_weight: Tensor[(768, 3072), float32], %hybridbertencoder2_transformer3_bertpositionwiseffn0_ffn_2_bias: Tensor[(768,), float32], %hybridbertencoder2_transformer3_bertpositionwiseffn0_bertlayernorm0_gamma: Tensor[(768,), float32], %hybridbertencoder2_transformer3_bertpositionwiseffn0_bertlayernorm0_beta: Tensor[(768,), float32], %hybridbertencoder2_transformer4_multiheadattentioncell0_query_weight: Tensor[(768, 768), float32], %hybridbertencoder2_transformer4_multiheadattentioncell0_query_bias: Tensor[(768,), float32], %hybridbertencoder2_transformer4_multiheadattentioncell0_key_weight: Tensor[(768, 768), float32], %hybridbertencoder2_transformer4_multiheadattentioncell0_key_bias: Tensor[(768,), float32], %hybridbertencoder2_transformer4_multiheadattentioncell0_value_weight: Tensor[(768, 768), float32], %hybridbertencoder2_transformer4_multiheadattentioncell0_value_bias: Tensor[(768,), float32], %hybridbertencoder2_transformer4_proj_weight: Tensor[(768, 768), float32], %hybridbertencoder2_transformer4_proj_bias: Tensor[(768,), float32], %hybridbertencoder2_transformer4_bertlayernorm0_gamma: Tensor[(768,), float32], %hybridbertencoder2_transformer4_bertlayernorm0_beta: Tensor[(768,), float32], %hybridbertencoder2_transformer4_bertpositionwiseffn0_ffn_1_weight: Tensor[(3072, 768), float32], %hybridbertencoder2_transformer4_bertpositionwiseffn0_ffn_1_bias: Tensor[(3072,), float32], %hybridbertencoder2_transformer4_bertpositionwiseffn0_ffn_2_weight: Tensor[(768, 3072), float32], %hybridbertencoder2_transformer4_bertpositionwiseffn0_ffn_2_bias: Tensor[(768,), float32], %hybridbertencoder2_transformer4_bertpositionwiseffn0_bertlayernorm0_gamma: Tensor[(768,), float32], %hybridbertencoder2_transformer4_bertpositionwiseffn0_bertlayernorm0_beta: Tensor[(768,), float32], %hybridbertencoder2_transformer5_multiheadattentioncell0_query_weight: Tensor[(768, 768), float32], %hybridbertencoder2_transformer5_multiheadattentioncell0_query_bias: Tensor[(768,), float32], %hybridbertencoder2_transformer5_multiheadattentioncell0_key_weight: Tensor[(768, 768), float32], %hybridbertencoder2_transformer5_multiheadattentioncell0_key_bias: Tensor[(768,), float32], %hybridbertencoder2_transformer5_multiheadattentioncell0_value_weight: Tensor[(768, 768), float32], %hybridbertencoder2_transformer5_multiheadattentioncell0_value_bias: Tensor[(768,), float32], %hybridbertencoder2_transformer5_proj_weight: Tensor[(768, 768), float32], %hybridbertencoder2_transformer5_proj_bias: Tensor[(768,), float32], %hybridbertencoder2_transformer5_bertlayernorm0_gamma: Tensor[(768,), float32], %hybridbertencoder2_transformer5_bertlayernorm0_beta: Tensor[(768,), float32], %hybridbertencoder2_transformer5_bertpositionwiseffn0_ffn_1_weight: Tensor[(3072, 768), float32], %hybridbertencoder2_transformer5_bertpositionwiseffn0_ffn_1_bias: Tensor[(3072,), float32], %hybridbertencoder2_transformer5_bertpositionwiseffn0_ffn_2_weight: Tensor[(768, 3072), float32], %hybridbertencoder2_transformer5_bertpositionwiseffn0_ffn_2_bias: Tensor[(768,), float32], %hybridbertencoder2_transformer5_bertpositionwiseffn0_bertlayernorm0_gamma: Tensor[(768,), float32], %hybridbertencoder2_transformer5_bertpositionwiseffn0_bertlayernorm0_beta: Tensor[(768,), float32], %hybridbertencoder2_transformer6_multiheadattentioncell0_query_weight: Tensor[(768, 768), float32], %hybridbertencoder2_transformer6_multiheadattentioncell0_query_bias: Tensor[(768,), float32], %hybridbertencoder2_transformer6_multiheadattentioncell0_key_weight: Tensor[(768, 768), float32], %hybridbertencoder2_transformer6_multiheadattentioncell0_key_bias: Tensor[(768,), float32], %hybridbertencoder2_transformer6_multiheadattentioncell0_value_weight: Tensor[(768, 768), float32], %hybridbertencoder2_transformer6_multiheadattentioncell0_value_bias: Tensor[(768,), float32], %hybridbertencoder2_transformer6_proj_weight: Tensor[(768, 768), float32], %hybridbertencoder2_transformer6_proj_bias: Tensor[(768,), float32], %hybridbertencoder2_transformer6_bertlayernorm0_gamma: Tensor[(768,), float32], %hybridbertencoder2_transformer6_bertlayernorm0_beta: Tensor[(768,), float32], %hybridbertencoder2_transformer6_bertpositionwiseffn0_ffn_1_weight: Tensor[(3072, 768), float32], %hybridbertencoder2_transformer6_bertpositionwiseffn0_ffn_1_bias: Tensor[(3072,), float32], %hybridbertencoder2_transformer6_bertpositionwiseffn0_ffn_2_weight: Tensor[(768, 3072), float32], %hybridbertencoder2_transformer6_bertpositionwiseffn0_ffn_2_bias: Tensor[(768,), float32], %hybridbertencoder2_transformer6_bertpositionwiseffn0_bertlayernorm0_gamma: Tensor[(768,), float32], %hybridbertencoder2_transformer6_bertpositionwiseffn0_bertlayernorm0_beta: Tensor[(768,), float32], %hybridbertencoder2_transformer7_multiheadattentioncell0_query_weight: Tensor[(768, 768), float32], %hybridbertencoder2_transformer7_multiheadattentioncell0_query_bias: Tensor[(768,), float32], %hybridbertencoder2_transformer7_multiheadattentioncell0_key_weight: Tensor[(768, 768), float32], %hybridbertencoder2_transformer7_multiheadattentioncell0_key_bias: Tensor[(768,), float32], %hybridbertencoder2_transformer7_multiheadattentioncell0_value_weight: Tensor[(768, 768), float32], %hybridbertencoder2_transformer7_multiheadattentioncell0_value_bias: Tensor[(768,), float32], %hybridbertencoder2_transformer7_proj_weight: Tensor[(768, 768), float32], %hybridbertencoder2_transformer7_proj_bias: Tensor[(768,), float32], %hybridbertencoder2_transformer7_bertlayernorm0_gamma: Tensor[(768,), float32], %hybridbertencoder2_transformer7_bertlayernorm0_beta: Tensor[(768,), float32], %hybridbertencoder2_transformer7_bertpositionwiseffn0_ffn_1_weight: Tensor[(3072, 768), float32], %hybridbertencoder2_transformer7_bertpositionwiseffn0_ffn_1_bias: Tensor[(3072,), float32], %hybridbertencoder2_transformer7_bertpositionwiseffn0_ffn_2_weight: Tensor[(768, 3072), float32], %hybridbertencoder2_transformer7_bertpositionwiseffn0_ffn_2_bias: Tensor[(768,), float32], %hybridbertencoder2_transformer7_bertpositionwiseffn0_bertlayernorm0_gamma: Tensor[(768,), float32], %hybridbertencoder2_transformer7_bertpositionwiseffn0_bertlayernorm0_beta: Tensor[(768,), float32], %hybridbertencoder2_transformer8_multiheadattentioncell0_query_weight: Tensor[(768, 768), float32], %hybridbertencoder2_transformer8_multiheadattentioncell0_query_bias: Tensor[(768,), float32], %hybridbertencoder2_transformer8_multiheadattentioncell0_key_weight: Tensor[(768, 768), float32], %hybridbertencoder2_transformer8_multiheadattentioncell0_key_bias: Tensor[(768,), float32], %hybridbertencoder2_transformer8_multiheadattentioncell0_value_weight: Tensor[(768, 768), float32], %hybridbertencoder2_transformer8_multiheadattentioncell0_value_bias: Tensor[(768,), float32], %hybridbertencoder2_transformer8_proj_weight: Tensor[(768, 768), float32], %hybridbertencoder2_transformer8_proj_bias: Tensor[(768,), float32], %hybridbertencoder2_transformer8_bertlayernorm0_gamma: Tensor[(768,), float32], %hybridbertencoder2_transformer8_bertlayernorm0_beta: Tensor[(768,), float32], %hybridbertencoder2_transformer8_bertpositionwiseffn0_ffn_1_weight: Tensor[(3072, 768), float32], %hybridbertencoder2_transformer8_bertpositionwiseffn0_ffn_1_bias: Tensor[(3072,), float32], %hybridbertencoder2_transformer8_bertpositionwiseffn0_ffn_2_weight: Tensor[(768, 3072), float32], %hybridbertencoder2_transformer8_bertpositionwiseffn0_ffn_2_bias: Tensor[(768,), float32], %hybridbertencoder2_transformer8_bertpositionwiseffn0_bertlayernorm0_gamma: Tensor[(768,), float32], %hybridbertencoder2_transformer8_bertpositionwiseffn0_bertlayernorm0_beta: Tensor[(768,), float32], %hybridbertencoder2_transformer9_multiheadattentioncell0_query_weight: Tensor[(768, 768), float32], %hybridbertencoder2_transformer9_multiheadattentioncell0_query_bias: Tensor[(768,), float32], %hybridbertencoder2_transformer9_multiheadattentioncell0_key_weight: Tensor[(768, 768), float32], %hybridbertencoder2_transformer9_multiheadattentioncell0_key_bias: Tensor[(768,), float32], %hybridbertencoder2_transformer9_multiheadattentioncell0_value_weight: Tensor[(768, 768), float32], %hybridbertencoder2_transformer9_multiheadattentioncell0_value_bias: Tensor[(768,), float32], %hybridbertencoder2_transformer9_proj_weight: Tensor[(768, 768), float32], %hybridbertencoder2_transformer9_proj_bias: Tensor[(768,), float32], %hybridbertencoder2_transformer9_bertlayernorm0_gamma: Tensor[(768,), float32], %hybridbertencoder2_transformer9_bertlayernorm0_beta: Tensor[(768,), float32], %hybridbertencoder2_transformer9_bertpositionwiseffn0_ffn_1_weight: Tensor[(3072, 768), float32], %hybridbertencoder2_transformer9_bertpositionwiseffn0_ffn_1_bias: Tensor[(3072,), float32], %hybridbertencoder2_transformer9_bertpositionwiseffn0_ffn_2_weight: Tensor[(768, 3072), float32], %hybridbertencoder2_transformer9_bertpositionwiseffn0_ffn_2_bias: Tensor[(768,), float32], %hybridbertencoder2_transformer9_bertpositionwiseffn0_bertlayernorm0_gamma: Tensor[(768,), float32], %hybridbertencoder2_transformer9_bertpositionwiseffn0_bertlayernorm0_beta: Tensor[(768,), float32], %hybridbertencoder2_transformer10_multiheadattentioncell0_query_weight: Tensor[(768, 768), float32], %hybridbertencoder2_transformer10_multiheadattentioncell0_query_bias: Tensor[(768,), float32], %hybridbertencoder2_transformer10_multiheadattentioncell0_key_weight: Tensor[(768, 768), float32], %hybridbertencoder2_transformer10_multiheadattentioncell0_key_bias: Tensor[(768,), float32], %hybridbertencoder2_transformer10_multiheadattentioncell0_value_weight: Tensor[(768, 768), float32], %hybridbertencoder2_transformer10_multiheadattentioncell0_value_bias: Tensor[(768,), float32], %hybridbertencoder2_transformer10_proj_weight: Tensor[(768, 768), float32], %hybridbertencoder2_transformer10_proj_bias: Tensor[(768,), float32], %hybridbertencoder2_transformer10_bertlayernorm0_gamma: Tensor[(768,), float32], %hybridbertencoder2_transformer10_bertlayernorm0_beta: Tensor[(768,), float32], %hybridbertencoder2_transformer10_bertpositionwiseffn0_ffn_1_weight: Tensor[(3072, 768), float32], %hybridbertencoder2_transformer10_bertpositionwiseffn0_ffn_1_bias: Tensor[(3072,), float32], %hybridbertencoder2_transformer10_bertpositionwiseffn0_ffn_2_weight: Tensor[(768, 3072), float32], %hybridbertencoder2_transformer10_bertpositionwiseffn0_ffn_2_bias: Tensor[(768,), float32], %hybridbertencoder2_transformer10_bertpositionwiseffn0_bertlayernorm0_gamma: Tensor[(768,), float32], %hybridbertencoder2_transformer10_bertpositionwiseffn0_bertlayernorm0_beta: Tensor[(768,), float32], %hybridbertencoder2_transformer11_multiheadattentioncell0_query_weight: Tensor[(768, 768), float32], %hybridbertencoder2_transformer11_multiheadattentioncell0_query_bias: Tensor[(768,), float32], %hybridbertencoder2_transformer11_multiheadattentioncell0_key_weight: Tensor[(768, 768), float32], %hybridbertencoder2_transformer11_multiheadattentioncell0_key_bias: Tensor[(768,), float32], %hybridbertencoder2_transformer11_multiheadattentioncell0_value_weight: Tensor[(768, 768), float32], %hybridbertencoder2_transformer11_multiheadattentioncell0_value_bias: Tensor[(768,), float32], %hybridbertencoder2_transformer11_proj_weight: Tensor[(768, 768), float32], %hybridbertencoder2_transformer11_proj_bias: Tensor[(768,), float32], %hybridbertencoder2_transformer11_bertlayernorm0_gamma: Tensor[(768,), float32], %hybridbertencoder2_transformer11_bertlayernorm0_beta: Tensor[(768,), float32], %hybridbertencoder2_transformer11_bertpositionwiseffn0_ffn_1_weight: Tensor[(3072, 768), float32], %hybridbertencoder2_transformer11_bertpositionwiseffn0_ffn_1_bias: Tensor[(3072,), float32], %hybridbertencoder2_transformer11_bertpositionwiseffn0_ffn_2_weight: Tensor[(768, 3072), float32], %hybridbertencoder2_transformer11_bertpositionwiseffn0_ffn_2_bias: Tensor[(768,), float32], %hybridbertencoder2_transformer11_bertpositionwiseffn0_bertlayernorm0_gamma: Tensor[(768,), float32], %hybridbertencoder2_transformer11_bertpositionwiseffn0_bertlayernorm0_beta: Tensor[(768,), float32], %hybridbertforqa2_dense0_weight: Tensor[(2, 768), float32], %hybridbertforqa2_dense0_bias: Tensor[(2,), float32]) -> Tensor[(1, 256, 2), float32] {\n",
      "  %0 = arange(, start=0, stop=256f, dtype=\"float32\") /* ty=Tensor[(256,), float32] */\n",
      "  %1 = reshape(%0, newshape=[1, -1]) /* ty=Tensor[(1, 256), float32] */\n",
      "  %2 = reshape(%data2, newshape=[-1, 1]) /* ty=Tensor[(1, 1), float32] */\n",
      "  %3 = less(%1, %2) /* ty=Tensor[(1, 256), bool] */\n",
      "  %4 = cast(%3, dtype=\"float32\") /* ty=Tensor[(1, 256), float32] */\n",
      "  %5 = expand_dims(%4, axis=1) /* ty=Tensor[(1, 1, 256), float32] */\n",
      "  %6 = broadcast_to(%5, meta[relay.attrs.InitOpAttrs][0]) /* ty=Tensor[(1, 256, 256), float32] */\n",
      "  %7 = expand_dims(%6, axis=1) /* ty=Tensor[(1, 1, 256, 256), float32] */\n",
      "  %8 = broadcast_to(%7, meta[relay.attrs.InitOpAttrs][1]) /* ty=Tensor[(1, 12, 256, 256), float32] */\n",
      "  %9 = _contrib_reverse_reshape(%8, newshape=[-1, 0, 0], reverse=True) /* ty=Tensor[(12, 256, 256), float32] */\n",
      "  %10 = expand_dims(%6, axis=1) /* ty=Tensor[(1, 1, 256, 256), float32] */\n",
      "  %11 = broadcast_to(%10, meta[relay.attrs.InitOpAttrs][2]) /* ty=Tensor[(1, 12, 256, 256), float32] */\n",
      "  %12 = _contrib_reverse_reshape(%11, newshape=[-1, 0, 0], reverse=True) /* ty=Tensor[(12, 256, 256), float32] */\n",
      "  %13 = expand_dims(%6, axis=1) /* ty=Tensor[(1, 1, 256, 256), float32] */\n",
      "  %14 = broadcast_to(%13, meta[relay.attrs.InitOpAttrs][3]) /* ty=Tensor[(1, 12, 256, 256), float32] */\n",
      "  %15 = _contrib_reverse_reshape(%14, newshape=[-1, 0, 0], reverse=True) /* ty=Tensor[(12, 256, 256), float32] */\n",
      "  %16 = expand_dims(%6, axis=1) /* ty=Tensor[(1, 1, 256, 256), float32] */\n",
      "  %17 = broadcast_to(%16, meta[relay.attrs.InitOpAttrs][4]) /* ty=Tensor[(1, 12, 256, 256), float32] */\n",
      "  %18 = _contrib_reverse_reshape(%17, newshape=[-1, 0, 0], reverse=True) /* ty=Tensor[(12, 256, 256), float32] */\n",
      "  %19 = expand_dims(%6, axis=1) /* ty=Tensor[(1, 1, 256, 256), float32] */\n",
      "  %20 = broadcast_to(%19, meta[relay.attrs.InitOpAttrs][5]) /* ty=Tensor[(1, 12, 256, 256), float32] */\n",
      "  %21 = _contrib_reverse_reshape(%20, newshape=[-1, 0, 0], reverse=True) /* ty=Tensor[(12, 256, 256), float32] */\n",
      "  %22 = expand_dims(%6, axis=1) /* ty=Tensor[(1, 1, 256, 256), float32] */\n",
      "  %23 = broadcast_to(%22, meta[relay.attrs.InitOpAttrs][6]) /* ty=Tensor[(1, 12, 256, 256), float32] */\n",
      "  %24 = _contrib_reverse_reshape(%23, newshape=[-1, 0, 0], reverse=True) /* ty=Tensor[(12, 256, 256), float32] */\n",
      "  %25 = expand_dims(%6, axis=1) /* ty=Tensor[(1, 1, 256, 256), float32] */\n",
      "  %26 = broadcast_to(%25, meta[relay.attrs.InitOpAttrs][7]) /* ty=Tensor[(1, 12, 256, 256), float32] */\n",
      "  %27 = _contrib_reverse_reshape(%26, newshape=[-1, 0, 0], reverse=True) /* ty=Tensor[(12, 256, 256), float32] */\n",
      "  %28 = expand_dims(%6, axis=1) /* ty=Tensor[(1, 1, 256, 256), float32] */\n",
      "  %29 = broadcast_to(%28, meta[relay.attrs.InitOpAttrs][8]) /* ty=Tensor[(1, 12, 256, 256), float32] */\n",
      "  %30 = _contrib_reverse_reshape(%29, newshape=[-1, 0, 0], reverse=True) /* ty=Tensor[(12, 256, 256), float32] */\n",
      "  %31 = expand_dims(%6, axis=1) /* ty=Tensor[(1, 1, 256, 256), float32] */\n",
      "  %32 = broadcast_to(%31, meta[relay.attrs.InitOpAttrs][9]) /* ty=Tensor[(1, 12, 256, 256), float32] */\n",
      "  %33 = _contrib_reverse_reshape(%32, newshape=[-1, 0, 0], reverse=True) /* ty=Tensor[(12, 256, 256), float32] */\n",
      "  %34 = expand_dims(%6, axis=1) /* ty=Tensor[(1, 1, 256, 256), float32] */\n",
      "  %35 = broadcast_to(%34, meta[relay.attrs.InitOpAttrs][10]) /* ty=Tensor[(1, 12, 256, 256), float32] */\n",
      "  %36 = _contrib_reverse_reshape(%35, newshape=[-1, 0, 0], reverse=True) /* ty=Tensor[(12, 256, 256), float32] */\n",
      "  %37 = expand_dims(%6, axis=1) /* ty=Tensor[(1, 1, 256, 256), float32] */\n",
      "  %38 = broadcast_to(%37, meta[relay.attrs.InitOpAttrs][11]) /* ty=Tensor[(1, 12, 256, 256), float32] */\n",
      "  %39 = _contrib_reverse_reshape(%38, newshape=[-1, 0, 0], reverse=True) /* ty=Tensor[(12, 256, 256), float32] */\n",
      "  %40 = expand_dims(%6, axis=1) /* ty=Tensor[(1, 1, 256, 256), float32] */\n",
      "  %41 = broadcast_to(%40, meta[relay.attrs.InitOpAttrs][12]) /* ty=Tensor[(1, 12, 256, 256), float32] */\n",
      "  %42 = _contrib_reverse_reshape(%41, newshape=[-1, 0, 0], reverse=True) /* ty=Tensor[(12, 256, 256), float32] */\n",
      "  %43 = cast(%data0, dtype=\"int32\") /* ty=Tensor[(1, 256), int32] */\n",
      "  %44 = take(%hybridbertmodel2_word_embed_embedding0_weight, %43, axis=0) /* ty=Tensor[(1, 256, 768), float32] */\n",
      "  %45 = nn.dropout(%44, rate=0.1) /* ty=(Tensor[(1, 256, 768), float32], Tensor[(1, 256, 768), float32]) */\n",
      "  %46 = %45.0\n",
      "  %47 = cast(%data1, dtype=\"int32\") /* ty=Tensor[(1, 256), int32] */\n",
      "  %48 = take(%hybridbertmodel2_token_type_embed_embedding0_weight, %47, axis=0) /* ty=Tensor[(1, 256, 768), float32] */\n",
      "  %49 = nn.dropout(%48, rate=0.1) /* ty=(Tensor[(1, 256, 768), float32], Tensor[(1, 256, 768), float32]) */\n",
      "  %50 = %49.0\n",
      "  %51 = add(%46, %50) /* ty=Tensor[(1, 256, 768), float32] */\n",
      "  %52 = arange(, start=0, stop=256f, dtype=\"float32\") /* ty=Tensor[(256,), float32] */\n",
      "  %53 = cast(%52, dtype=\"int32\") /* ty=Tensor[(256,), int32] */\n",
      "  %54 = take(%hybridbertencoder2_position_weight, %53, axis=0) /* ty=Tensor[(256, 768), float32] */\n",
      "  %55 = expand_dims(%54, axis=0) /* ty=Tensor[(1, 256, 768), float32] */\n",
      "  %56 = add(%51, %55) /* ty=Tensor[(1, 256, 768), float32] */\n",
      "  %57 = nn.dropout(%56, rate=0.1) /* ty=(Tensor[(1, 256, 768), float32], Tensor[(1, 256, 768), float32]) */\n",
      "  %58 = %57.0\n",
      "  %59 = nn.layer_norm(%58, %hybridbertencoder2_bertlayernorm0_gamma, %hybridbertencoder2_bertlayernorm0_beta, epsilon=1e-12) /* ty=Tensor[(1, 256, 768), float32] */\n",
      "  %60 = _contrib_reverse_reshape(%59, newshape=[-1, 0], reverse=True) /* ty=Tensor[(256, 768), float32] */\n",
      "  %61 = nn.dense(%60, %hybridbertencoder2_transformer0_multiheadattentioncell0_query_weight, units=768) /* ty=Tensor[(256, 768), float32] */\n",
      "  %62 = nn.bias_add(%61, %hybridbertencoder2_transformer0_multiheadattentioncell0_query_bias, axis=-1) /* ty=Tensor[(256, 768), float32] */\n",
      "  %63 = reshape(%62, newshape=[1, 256, 768]) /* ty=Tensor[(1, 256, 768), float32] */\n",
      "  %64 = reshape(%63, newshape=[0, 0, 12, -1]) /* ty=Tensor[(1, 256, 12, 64), float32] */\n",
      "  %65 = transpose(%64, axes=[0, 2, 1, 3]) /* ty=Tensor[(1, 12, 256, 64), float32] */\n",
      "  %66 = _contrib_reverse_reshape(%65, newshape=[-1, 0, 0], reverse=True) /* ty=Tensor[(12, 256, 64), float32] */\n",
      "  %67 = shape_of(%66, dtype=\"int32\") /* ty=Tensor[(3,), int32] */\n",
      "  %68 = take(%67, 2 /* ty=int32 */) /* ty=int32 */\n",
      "  %69 = cast(%68, dtype=\"float32\") /* ty=float32 */\n",
      "  %70 = sqrt(%69) /* ty=float32 */\n",
      "  %71 = divide(%66, %70) /* ty=Tensor[(12, 256, 64), float32] */\n",
      "  %72 = _contrib_reverse_reshape(%59, newshape=[-1, 0], reverse=True) /* ty=Tensor[(256, 768), float32] */\n",
      "  %73 = nn.dense(%72, %hybridbertencoder2_transformer0_multiheadattentioncell0_key_weight, units=768) /* ty=Tensor[(256, 768), float32] */\n",
      "  %74 = nn.bias_add(%73, %hybridbertencoder2_transformer0_multiheadattentioncell0_key_bias, axis=-1) /* ty=Tensor[(256, 768), float32] */\n",
      "  %75 = reshape(%74, newshape=[1, 256, 768]) /* ty=Tensor[(1, 256, 768), float32] */\n",
      "  %76 = reshape(%75, newshape=[0, 0, 12, -1]) /* ty=Tensor[(1, 256, 12, 64), float32] */\n",
      "  %77 = transpose(%76, axes=[0, 2, 1, 3]) /* ty=Tensor[(1, 12, 256, 64), float32] */\n",
      "  %78 = _contrib_reverse_reshape(%77, newshape=[-1, 0, 0], reverse=True) /* ty=Tensor[(12, 256, 64), float32] */\n",
      "  %79 = nn.batch_matmul(%71, %78) /* ty=Tensor[(12, 256, 256), float32] */\n",
      "  %80 = ones_like(%79) /* ty=Tensor[(12, 256, 256), float32] */\n",
      "  %81 = multiply(%80, -1e+18f /* ty=float32 */) /* ty=Tensor[(12, 256, 256), float32] */\n",
      "  %82 = where(%42, %79, %81) /* ty=Tensor[(12, 256, 256), float32] */\n",
      "  %83 = nn.softmax(%82) /* ty=Tensor[(12, 256, 256), float32] */\n",
      "  %84 = multiply(%83, %42) /* ty=Tensor[(12, 256, 256), float32] */\n",
      "  %85 = nn.dropout(%84, rate=0.1) /* ty=(Tensor[(12, 256, 256), float32], Tensor[(12, 256, 256), float32]) */\n",
      "  %86 = %85.0\n",
      "  %87 = _contrib_reverse_reshape(%86, newshape=[-1, 12, 0, 0], reverse=True) /* ty=Tensor[(1, 12, 256, 256), float32] */\n",
      "  %88 = _contrib_reverse_reshape(%87, newshape=[-1, 0, 0], reverse=True) /* ty=Tensor[(12, 256, 256), float32] */\n",
      "  %89 = _contrib_reverse_reshape(%59, newshape=[-1, 0], reverse=True) /* ty=Tensor[(256, 768), float32] */\n",
      "  %90 = nn.dense(%89, %hybridbertencoder2_transformer0_multiheadattentioncell0_value_weight, units=768) /* ty=Tensor[(256, 768), float32] */\n",
      "  %91 = nn.bias_add(%90, %hybridbertencoder2_transformer0_multiheadattentioncell0_value_bias, axis=-1) /* ty=Tensor[(256, 768), float32] */\n",
      "  %92 = reshape(%91, newshape=[1, 256, 768]) /* ty=Tensor[(1, 256, 768), float32] */\n",
      "  %93 = reshape(%92, newshape=[0, 0, 12, -1]) /* ty=Tensor[(1, 256, 12, 64), float32] */\n",
      "  %94 = transpose(%93, axes=[0, 2, 1, 3]) /* ty=Tensor[(1, 12, 256, 64), float32] */\n",
      "  %95 = _contrib_reverse_reshape(%94, newshape=[-1, 0, 0], reverse=True) /* ty=Tensor[(12, 256, 64), float32] */\n",
      "  %96 = transpose(%95, axes=[0, 2, 1]) /* ty=Tensor[(12, 64, 256), float32] */\n",
      "  %97 = nn.batch_matmul(%88, %96) /* ty=Tensor[(12, 256, 64), float32] */\n",
      "  %98 = _contrib_reverse_reshape(%97, newshape=[-1, 12, 0, 0], reverse=True) /* ty=Tensor[(1, 12, 256, 64), float32] */\n",
      "  %99 = transpose(%98, axes=[0, 2, 1, 3]) /* ty=Tensor[(1, 256, 12, 64), float32] */\n",
      "  %100 = reshape(%99, newshape=[0, 0, -1]) /* ty=Tensor[(1, 256, 768), float32] */\n",
      "  %101 = _contrib_reverse_reshape(%100, newshape=[-1, 0], reverse=True) /* ty=Tensor[(256, 768), float32] */\n",
      "  %102 = nn.dense(%101, %hybridbertencoder2_transformer0_proj_weight, units=768) /* ty=Tensor[(256, 768), float32] */\n",
      "  %103 = nn.bias_add(%102, %hybridbertencoder2_transformer0_proj_bias, axis=-1) /* ty=Tensor[(256, 768), float32] */\n",
      "  %104 = reshape(%103, newshape=[1, 256, 768]) /* ty=Tensor[(1, 256, 768), float32] */\n",
      "  %105 = nn.dropout(%104, rate=0.1) /* ty=(Tensor[(1, 256, 768), float32], Tensor[(1, 256, 768), float32]) */\n",
      "  %106 = %105.0\n",
      "  %107 = add(%106, %59) /* ty=Tensor[(1, 256, 768), float32] */\n",
      "  %108 = nn.layer_norm(%107, %hybridbertencoder2_transformer0_bertlayernorm0_gamma, %hybridbertencoder2_transformer0_bertlayernorm0_beta, epsilon=1e-12) /* ty=Tensor[(1, 256, 768), float32] */\n",
      "  %109 = _contrib_reverse_reshape(%108, newshape=[-1, 0], reverse=True) /* ty=Tensor[(256, 768), float32] */\n",
      "  %110 = nn.dense(%109, %hybridbertencoder2_transformer0_bertpositionwiseffn0_ffn_1_weight, units=3072) /* ty=Tensor[(256, 3072), float32] */\n",
      "  %111 = nn.bias_add(%110, %hybridbertencoder2_transformer0_bertpositionwiseffn0_ffn_1_bias, axis=-1) /* ty=Tensor[(256, 3072), float32] */\n",
      "  %112 = reshape(%111, newshape=[1, 256, 3072]) /* ty=Tensor[(1, 256, 3072), float32] */\n",
      "  %113 = multiply(%112, 0.5f /* ty=float32 */) /* ty=Tensor[(1, 256, 3072), float32] */\n",
      "  %114 = divide(%112, 1.41421f /* ty=float32 */) /* ty=Tensor[(1, 256, 3072), float32] */\n",
      "  %115 = erf(%114) /* ty=Tensor[(1, 256, 3072), float32] */\n",
      "  %116 = add(%115, 1f /* ty=float32 */) /* ty=Tensor[(1, 256, 3072), float32] */\n",
      "  %117 = multiply(%113, %116) /* ty=Tensor[(1, 256, 3072), float32] */\n",
      "  %118 = _contrib_reverse_reshape(%117, newshape=[-1, 0], reverse=True) /* ty=Tensor[(256, 3072), float32] */\n",
      "  %119 = nn.dense(%118, %hybridbertencoder2_transformer0_bertpositionwiseffn0_ffn_2_weight, units=768) /* ty=Tensor[(256, 768), float32] */\n",
      "  %120 = nn.bias_add(%119, %hybridbertencoder2_transformer0_bertpositionwiseffn0_ffn_2_bias, axis=-1) /* ty=Tensor[(256, 768), float32] */\n",
      "  %121 = reshape(%120, newshape=[1, 256, 768]) /* ty=Tensor[(1, 256, 768), float32] */\n",
      "  %122 = nn.dropout(%121, rate=0.1) /* ty=(Tensor[(1, 256, 768), float32], Tensor[(1, 256, 768), float32]) */\n",
      "  %123 = %122.0\n",
      "  %124 = add(%123, %108) /* ty=Tensor[(1, 256, 768), float32] */\n",
      "  %125 = nn.layer_norm(%124, %hybridbertencoder2_transformer0_bertpositionwiseffn0_bertlayernorm0_gamma, %hybridbertencoder2_transformer0_bertpositionwiseffn0_bertlayernorm0_beta, epsilon=1e-12) /* ty=Tensor[(1, 256, 768), float32] */\n",
      "  %126 = _contrib_reverse_reshape(%125, newshape=[-1, 0], reverse=True) /* ty=Tensor[(256, 768), float32] */\n",
      "  %127 = nn.dense(%126, %hybridbertencoder2_transformer1_multiheadattentioncell0_query_weight, units=768) /* ty=Tensor[(256, 768), float32] */\n",
      "  %128 = nn.bias_add(%127, %hybridbertencoder2_transformer1_multiheadattentioncell0_query_bias, axis=-1) /* ty=Tensor[(256, 768), float32] */\n",
      "  %129 = reshape(%128, newshape=[1, 256, 768]) /* ty=Tensor[(1, 256, 768), float32] */\n",
      "  %130 = reshape(%129, newshape=[0, 0, 12, -1]) /* ty=Tensor[(1, 256, 12, 64), float32] */\n",
      "  %131 = transpose(%130, axes=[0, 2, 1, 3]) /* ty=Tensor[(1, 12, 256, 64), float32] */\n",
      "  %132 = _contrib_reverse_reshape(%131, newshape=[-1, 0, 0], reverse=True) /* ty=Tensor[(12, 256, 64), float32] */\n",
      "  %133 = shape_of(%132, dtype=\"int32\") /* ty=Tensor[(3,), int32] */\n",
      "  %134 = take(%133, 2 /* ty=int32 */) /* ty=int32 */\n",
      "  %135 = cast(%134, dtype=\"float32\") /* ty=float32 */\n",
      "  %136 = sqrt(%135) /* ty=float32 */\n",
      "  %137 = divide(%132, %136) /* ty=Tensor[(12, 256, 64), float32] */\n",
      "  %138 = _contrib_reverse_reshape(%125, newshape=[-1, 0], reverse=True) /* ty=Tensor[(256, 768), float32] */\n",
      "  %139 = nn.dense(%138, %hybridbertencoder2_transformer1_multiheadattentioncell0_key_weight, units=768) /* ty=Tensor[(256, 768), float32] */\n",
      "  %140 = nn.bias_add(%139, %hybridbertencoder2_transformer1_multiheadattentioncell0_key_bias, axis=-1) /* ty=Tensor[(256, 768), float32] */\n",
      "  %141 = reshape(%140, newshape=[1, 256, 768]) /* ty=Tensor[(1, 256, 768), float32] */\n",
      "  %142 = reshape(%141, newshape=[0, 0, 12, -1]) /* ty=Tensor[(1, 256, 12, 64), float32] */\n",
      "  %143 = transpose(%142, axes=[0, 2, 1, 3]) /* ty=Tensor[(1, 12, 256, 64), float32] */\n",
      "  %144 = _contrib_reverse_reshape(%143, newshape=[-1, 0, 0], reverse=True) /* ty=Tensor[(12, 256, 64), float32] */\n",
      "  %145 = nn.batch_matmul(%137, %144) /* ty=Tensor[(12, 256, 256), float32] */\n",
      "  %146 = ones_like(%145) /* ty=Tensor[(12, 256, 256), float32] */\n",
      "  %147 = multiply(%146, -1e+18f /* ty=float32 */) /* ty=Tensor[(12, 256, 256), float32] */\n",
      "  %148 = where(%39, %145, %147) /* ty=Tensor[(12, 256, 256), float32] */\n",
      "  %149 = nn.softmax(%148) /* ty=Tensor[(12, 256, 256), float32] */\n",
      "  %150 = multiply(%149, %39) /* ty=Tensor[(12, 256, 256), float32] */\n",
      "  %151 = nn.dropout(%150, rate=0.1) /* ty=(Tensor[(12, 256, 256), float32], Tensor[(12, 256, 256), float32]) */\n",
      "  %152 = %151.0\n",
      "  %153 = _contrib_reverse_reshape(%152, newshape=[-1, 12, 0, 0], reverse=True) /* ty=Tensor[(1, 12, 256, 256), float32] */\n",
      "  %154 = _contrib_reverse_reshape(%153, newshape=[-1, 0, 0], reverse=True) /* ty=Tensor[(12, 256, 256), float32] */\n",
      "  %155 = _contrib_reverse_reshape(%125, newshape=[-1, 0], reverse=True) /* ty=Tensor[(256, 768), float32] */\n",
      "  %156 = nn.dense(%155, %hybridbertencoder2_transformer1_multiheadattentioncell0_value_weight, units=768) /* ty=Tensor[(256, 768), float32] */\n",
      "  %157 = nn.bias_add(%156, %hybridbertencoder2_transformer1_multiheadattentioncell0_value_bias, axis=-1) /* ty=Tensor[(256, 768), float32] */\n",
      "  %158 = reshape(%157, newshape=[1, 256, 768]) /* ty=Tensor[(1, 256, 768), float32] */\n",
      "  %159 = reshape(%158, newshape=[0, 0, 12, -1]) /* ty=Tensor[(1, 256, 12, 64), float32] */\n",
      "  %160 = transpose(%159, axes=[0, 2, 1, 3]) /* ty=Tensor[(1, 12, 256, 64), float32] */\n",
      "  %161 = _contrib_reverse_reshape(%160, newshape=[-1, 0, 0], reverse=True) /* ty=Tensor[(12, 256, 64), float32] */\n",
      "  %162 = transpose(%161, axes=[0, 2, 1]) /* ty=Tensor[(12, 64, 256), float32] */\n",
      "  %163 = nn.batch_matmul(%154, %162) /* ty=Tensor[(12, 256, 64), float32] */\n",
      "  %164 = _contrib_reverse_reshape(%163, newshape=[-1, 12, 0, 0], reverse=True) /* ty=Tensor[(1, 12, 256, 64), float32] */\n",
      "  %165 = transpose(%164, axes=[0, 2, 1, 3]) /* ty=Tensor[(1, 256, 12, 64), float32] */\n",
      "  %166 = reshape(%165, newshape=[0, 0, -1]) /* ty=Tensor[(1, 256, 768), float32] */\n",
      "  %167 = _contrib_reverse_reshape(%166, newshape=[-1, 0], reverse=True) /* ty=Tensor[(256, 768), float32] */\n",
      "  %168 = nn.dense(%167, %hybridbertencoder2_transformer1_proj_weight, units=768) /* ty=Tensor[(256, 768), float32] */\n",
      "  %169 = nn.bias_add(%168, %hybridbertencoder2_transformer1_proj_bias, axis=-1) /* ty=Tensor[(256, 768), float32] */\n",
      "  %170 = reshape(%169, newshape=[1, 256, 768]) /* ty=Tensor[(1, 256, 768), float32] */\n",
      "  %171 = nn.dropout(%170, rate=0.1) /* ty=(Tensor[(1, 256, 768), float32], Tensor[(1, 256, 768), float32]) */\n",
      "  %172 = %171.0\n",
      "  %173 = add(%172, %125) /* ty=Tensor[(1, 256, 768), float32] */\n",
      "  %174 = nn.layer_norm(%173, %hybridbertencoder2_transformer1_bertlayernorm0_gamma, %hybridbertencoder2_transformer1_bertlayernorm0_beta, epsilon=1e-12) /* ty=Tensor[(1, 256, 768), float32] */\n",
      "  %175 = _contrib_reverse_reshape(%174, newshape=[-1, 0], reverse=True) /* ty=Tensor[(256, 768), float32] */\n",
      "  %176 = nn.dense(%175, %hybridbertencoder2_transformer1_bertpositionwiseffn0_ffn_1_weight, units=3072) /* ty=Tensor[(256, 3072), float32] */\n",
      "  %177 = nn.bias_add(%176, %hybridbertencoder2_transformer1_bertpositionwiseffn0_ffn_1_bias, axis=-1) /* ty=Tensor[(256, 3072), float32] */\n",
      "  %178 = reshape(%177, newshape=[1, 256, 3072]) /* ty=Tensor[(1, 256, 3072), float32] */\n",
      "  %179 = multiply(%178, 0.5f /* ty=float32 */) /* ty=Tensor[(1, 256, 3072), float32] */\n",
      "  %180 = divide(%178, 1.41421f /* ty=float32 */) /* ty=Tensor[(1, 256, 3072), float32] */\n",
      "  %181 = erf(%180) /* ty=Tensor[(1, 256, 3072), float32] */\n",
      "  %182 = add(%181, 1f /* ty=float32 */) /* ty=Tensor[(1, 256, 3072), float32] */\n",
      "  %183 = multiply(%179, %182) /* ty=Tensor[(1, 256, 3072), float32] */\n",
      "  %184 = _contrib_reverse_reshape(%183, newshape=[-1, 0], reverse=True) /* ty=Tensor[(256, 3072), float32] */\n",
      "  %185 = nn.dense(%184, %hybridbertencoder2_transformer1_bertpositionwiseffn0_ffn_2_weight, units=768) /* ty=Tensor[(256, 768), float32] */\n",
      "  %186 = nn.bias_add(%185, %hybridbertencoder2_transformer1_bertpositionwiseffn0_ffn_2_bias, axis=-1) /* ty=Tensor[(256, 768), float32] */\n",
      "  %187 = reshape(%186, newshape=[1, 256, 768]) /* ty=Tensor[(1, 256, 768), float32] */\n",
      "  %188 = nn.dropout(%187, rate=0.1) /* ty=(Tensor[(1, 256, 768), float32], Tensor[(1, 256, 768), float32]) */\n",
      "  %189 = %188.0\n",
      "  %190 = add(%189, %174) /* ty=Tensor[(1, 256, 768), float32] */\n",
      "  %191 = nn.layer_norm(%190, %hybridbertencoder2_transformer1_bertpositionwiseffn0_bertlayernorm0_gamma, %hybridbertencoder2_transformer1_bertpositionwiseffn0_bertlayernorm0_beta, epsilon=1e-12) /* ty=Tensor[(1, 256, 768), float32] */\n",
      "  %192 = _contrib_reverse_reshape(%191, newshape=[-1, 0], reverse=True) /* ty=Tensor[(256, 768), float32] */\n",
      "  %193 = nn.dense(%192, %hybridbertencoder2_transformer2_multiheadattentioncell0_query_weight, units=768) /* ty=Tensor[(256, 768), float32] */\n",
      "  %194 = nn.bias_add(%193, %hybridbertencoder2_transformer2_multiheadattentioncell0_query_bias, axis=-1) /* ty=Tensor[(256, 768), float32] */\n",
      "  %195 = reshape(%194, newshape=[1, 256, 768]) /* ty=Tensor[(1, 256, 768), float32] */\n",
      "  %196 = reshape(%195, newshape=[0, 0, 12, -1]) /* ty=Tensor[(1, 256, 12, 64), float32] */\n",
      "  %197 = transpose(%196, axes=[0, 2, 1, 3]) /* ty=Tensor[(1, 12, 256, 64), float32] */\n",
      "  %198 = _contrib_reverse_reshape(%197, newshape=[-1, 0, 0], reverse=True) /* ty=Tensor[(12, 256, 64), float32] */\n",
      "  %199 = shape_of(%198, dtype=\"int32\") /* ty=Tensor[(3,), int32] */\n",
      "  %200 = take(%199, 2 /* ty=int32 */) /* ty=int32 */\n",
      "  %201 = cast(%200, dtype=\"float32\") /* ty=float32 */\n",
      "  %202 = sqrt(%201) /* ty=float32 */\n",
      "  %203 = divide(%198, %202) /* ty=Tensor[(12, 256, 64), float32] */\n",
      "  %204 = _contrib_reverse_reshape(%191, newshape=[-1, 0], reverse=True) /* ty=Tensor[(256, 768), float32] */\n",
      "  %205 = nn.dense(%204, %hybridbertencoder2_transformer2_multiheadattentioncell0_key_weight, units=768) /* ty=Tensor[(256, 768), float32] */\n",
      "  %206 = nn.bias_add(%205, %hybridbertencoder2_transformer2_multiheadattentioncell0_key_bias, axis=-1) /* ty=Tensor[(256, 768), float32] */\n",
      "  %207 = reshape(%206, newshape=[1, 256, 768]) /* ty=Tensor[(1, 256, 768), float32] */\n",
      "  %208 = reshape(%207, newshape=[0, 0, 12, -1]) /* ty=Tensor[(1, 256, 12, 64), float32] */\n",
      "  %209 = transpose(%208, axes=[0, 2, 1, 3]) /* ty=Tensor[(1, 12, 256, 64), float32] */\n",
      "  %210 = _contrib_reverse_reshape(%209, newshape=[-1, 0, 0], reverse=True) /* ty=Tensor[(12, 256, 64), float32] */\n",
      "  %211 = nn.batch_matmul(%203, %210) /* ty=Tensor[(12, 256, 256), float32] */\n",
      "  %212 = ones_like(%211) /* ty=Tensor[(12, 256, 256), float32] */\n",
      "  %213 = multiply(%212, -1e+18f /* ty=float32 */) /* ty=Tensor[(12, 256, 256), float32] */\n",
      "  %214 = where(%36, %211, %213) /* ty=Tensor[(12, 256, 256), float32] */\n",
      "  %215 = nn.softmax(%214) /* ty=Tensor[(12, 256, 256), float32] */\n",
      "  %216 = multiply(%215, %36) /* ty=Tensor[(12, 256, 256), float32] */\n",
      "  %217 = nn.dropout(%216, rate=0.1) /* ty=(Tensor[(12, 256, 256), float32], Tensor[(12, 256, 256), float32]) */\n",
      "  %218 = %217.0\n",
      "  %219 = _contrib_reverse_reshape(%218, newshape=[-1, 12, 0, 0], reverse=True) /* ty=Tensor[(1, 12, 256, 256), float32] */\n",
      "  %220 = _contrib_reverse_reshape(%219, newshape=[-1, 0, 0], reverse=True) /* ty=Tensor[(12, 256, 256), float32] */\n",
      "  %221 = _contrib_reverse_reshape(%191, newshape=[-1, 0], reverse=True) /* ty=Tensor[(256, 768), float32] */\n",
      "  %222 = nn.dense(%221, %hybridbertencoder2_transformer2_multiheadattentioncell0_value_weight, units=768) /* ty=Tensor[(256, 768), float32] */\n",
      "  %223 = nn.bias_add(%222, %hybridbertencoder2_transformer2_multiheadattentioncell0_value_bias, axis=-1) /* ty=Tensor[(256, 768), float32] */\n",
      "  %224 = reshape(%223, newshape=[1, 256, 768]) /* ty=Tensor[(1, 256, 768), float32] */\n",
      "  %225 = reshape(%224, newshape=[0, 0, 12, -1]) /* ty=Tensor[(1, 256, 12, 64), float32] */\n",
      "  %226 = transpose(%225, axes=[0, 2, 1, 3]) /* ty=Tensor[(1, 12, 256, 64), float32] */\n",
      "  %227 = _contrib_reverse_reshape(%226, newshape=[-1, 0, 0], reverse=True) /* ty=Tensor[(12, 256, 64), float32] */\n",
      "  %228 = transpose(%227, axes=[0, 2, 1]) /* ty=Tensor[(12, 64, 256), float32] */\n",
      "  %229 = nn.batch_matmul(%220, %228) /* ty=Tensor[(12, 256, 64), float32] */\n",
      "  %230 = _contrib_reverse_reshape(%229, newshape=[-1, 12, 0, 0], reverse=True) /* ty=Tensor[(1, 12, 256, 64), float32] */\n",
      "  %231 = transpose(%230, axes=[0, 2, 1, 3]) /* ty=Tensor[(1, 256, 12, 64), float32] */\n",
      "  %232 = reshape(%231, newshape=[0, 0, -1]) /* ty=Tensor[(1, 256, 768), float32] */\n",
      "  %233 = _contrib_reverse_reshape(%232, newshape=[-1, 0], reverse=True) /* ty=Tensor[(256, 768), float32] */\n",
      "  %234 = nn.dense(%233, %hybridbertencoder2_transformer2_proj_weight, units=768) /* ty=Tensor[(256, 768), float32] */\n",
      "  %235 = nn.bias_add(%234, %hybridbertencoder2_transformer2_proj_bias, axis=-1) /* ty=Tensor[(256, 768), float32] */\n",
      "  %236 = reshape(%235, newshape=[1, 256, 768]) /* ty=Tensor[(1, 256, 768), float32] */\n",
      "  %237 = nn.dropout(%236, rate=0.1) /* ty=(Tensor[(1, 256, 768), float32], Tensor[(1, 256, 768), float32]) */\n",
      "  %238 = %237.0\n",
      "  %239 = add(%238, %191) /* ty=Tensor[(1, 256, 768), float32] */\n",
      "  %240 = nn.layer_norm(%239, %hybridbertencoder2_transformer2_bertlayernorm0_gamma, %hybridbertencoder2_transformer2_bertlayernorm0_beta, epsilon=1e-12) /* ty=Tensor[(1, 256, 768), float32] */\n",
      "  %241 = _contrib_reverse_reshape(%240, newshape=[-1, 0], reverse=True) /* ty=Tensor[(256, 768), float32] */\n",
      "  %242 = nn.dense(%241, %hybridbertencoder2_transformer2_bertpositionwiseffn0_ffn_1_weight, units=3072) /* ty=Tensor[(256, 3072), float32] */\n",
      "  %243 = nn.bias_add(%242, %hybridbertencoder2_transformer2_bertpositionwiseffn0_ffn_1_bias, axis=-1) /* ty=Tensor[(256, 3072), float32] */\n",
      "  %244 = reshape(%243, newshape=[1, 256, 3072]) /* ty=Tensor[(1, 256, 3072), float32] */\n",
      "  %245 = multiply(%244, 0.5f /* ty=float32 */) /* ty=Tensor[(1, 256, 3072), float32] */\n",
      "  %246 = divide(%244, 1.41421f /* ty=float32 */) /* ty=Tensor[(1, 256, 3072), float32] */\n",
      "  %247 = erf(%246) /* ty=Tensor[(1, 256, 3072), float32] */\n",
      "  %248 = add(%247, 1f /* ty=float32 */) /* ty=Tensor[(1, 256, 3072), float32] */\n",
      "  %249 = multiply(%245, %248) /* ty=Tensor[(1, 256, 3072), float32] */\n",
      "  %250 = _contrib_reverse_reshape(%249, newshape=[-1, 0], reverse=True) /* ty=Tensor[(256, 3072), float32] */\n",
      "  %251 = nn.dense(%250, %hybridbertencoder2_transformer2_bertpositionwiseffn0_ffn_2_weight, units=768) /* ty=Tensor[(256, 768), float32] */\n",
      "  %252 = nn.bias_add(%251, %hybridbertencoder2_transformer2_bertpositionwiseffn0_ffn_2_bias, axis=-1) /* ty=Tensor[(256, 768), float32] */\n",
      "  %253 = reshape(%252, newshape=[1, 256, 768]) /* ty=Tensor[(1, 256, 768), float32] */\n",
      "  %254 = nn.dropout(%253, rate=0.1) /* ty=(Tensor[(1, 256, 768), float32], Tensor[(1, 256, 768), float32]) */\n",
      "  %255 = %254.0\n",
      "  %256 = add(%255, %240) /* ty=Tensor[(1, 256, 768), float32] */\n",
      "  %257 = nn.layer_norm(%256, %hybridbertencoder2_transformer2_bertpositionwiseffn0_bertlayernorm0_gamma, %hybridbertencoder2_transformer2_bertpositionwiseffn0_bertlayernorm0_beta, epsilon=1e-12) /* ty=Tensor[(1, 256, 768), float32] */\n",
      "  %258 = _contrib_reverse_reshape(%257, newshape=[-1, 0], reverse=True) /* ty=Tensor[(256, 768), float32] */\n",
      "  %259 = nn.dense(%258, %hybridbertencoder2_transformer3_multiheadattentioncell0_query_weight, units=768) /* ty=Tensor[(256, 768), float32] */\n",
      "  %260 = nn.bias_add(%259, %hybridbertencoder2_transformer3_multiheadattentioncell0_query_bias, axis=-1) /* ty=Tensor[(256, 768), float32] */\n",
      "  %261 = reshape(%260, newshape=[1, 256, 768]) /* ty=Tensor[(1, 256, 768), float32] */\n",
      "  %262 = reshape(%261, newshape=[0, 0, 12, -1]) /* ty=Tensor[(1, 256, 12, 64), float32] */\n",
      "  %263 = transpose(%262, axes=[0, 2, 1, 3]) /* ty=Tensor[(1, 12, 256, 64), float32] */\n",
      "  %264 = _contrib_reverse_reshape(%263, newshape=[-1, 0, 0], reverse=True) /* ty=Tensor[(12, 256, 64), float32] */\n",
      "  %265 = shape_of(%264, dtype=\"int32\") /* ty=Tensor[(3,), int32] */\n",
      "  %266 = take(%265, 2 /* ty=int32 */) /* ty=int32 */\n",
      "  %267 = cast(%266, dtype=\"float32\") /* ty=float32 */\n",
      "  %268 = sqrt(%267) /* ty=float32 */\n",
      "  %269 = divide(%264, %268) /* ty=Tensor[(12, 256, 64), float32] */\n",
      "  %270 = _contrib_reverse_reshape(%257, newshape=[-1, 0], reverse=True) /* ty=Tensor[(256, 768), float32] */\n",
      "  %271 = nn.dense(%270, %hybridbertencoder2_transformer3_multiheadattentioncell0_key_weight, units=768) /* ty=Tensor[(256, 768), float32] */\n",
      "  %272 = nn.bias_add(%271, %hybridbertencoder2_transformer3_multiheadattentioncell0_key_bias, axis=-1) /* ty=Tensor[(256, 768), float32] */\n",
      "  %273 = reshape(%272, newshape=[1, 256, 768]) /* ty=Tensor[(1, 256, 768), float32] */\n",
      "  %274 = reshape(%273, newshape=[0, 0, 12, -1]) /* ty=Tensor[(1, 256, 12, 64), float32] */\n",
      "  %275 = transpose(%274, axes=[0, 2, 1, 3]) /* ty=Tensor[(1, 12, 256, 64), float32] */\n",
      "  %276 = _contrib_reverse_reshape(%275, newshape=[-1, 0, 0], reverse=True) /* ty=Tensor[(12, 256, 64), float32] */\n",
      "  %277 = nn.batch_matmul(%269, %276) /* ty=Tensor[(12, 256, 256), float32] */\n",
      "  %278 = ones_like(%277) /* ty=Tensor[(12, 256, 256), float32] */\n",
      "  %279 = multiply(%278, -1e+18f /* ty=float32 */) /* ty=Tensor[(12, 256, 256), float32] */\n",
      "  %280 = where(%33, %277, %279) /* ty=Tensor[(12, 256, 256), float32] */\n",
      "  %281 = nn.softmax(%280) /* ty=Tensor[(12, 256, 256), float32] */\n",
      "  %282 = multiply(%281, %33) /* ty=Tensor[(12, 256, 256), float32] */\n",
      "  %283 = nn.dropout(%282, rate=0.1) /* ty=(Tensor[(12, 256, 256), float32], Tensor[(12, 256, 256), float32]) */\n",
      "  %284 = %283.0\n",
      "  %285 = _contrib_reverse_reshape(%284, newshape=[-1, 12, 0, 0], reverse=True) /* ty=Tensor[(1, 12, 256, 256), float32] */\n",
      "  %286 = _contrib_reverse_reshape(%285, newshape=[-1, 0, 0], reverse=True) /* ty=Tensor[(12, 256, 256), float32] */\n",
      "  %287 = _contrib_reverse_reshape(%257, newshape=[-1, 0], reverse=True) /* ty=Tensor[(256, 768), float32] */\n",
      "  %288 = nn.dense(%287, %hybridbertencoder2_transformer3_multiheadattentioncell0_value_weight, units=768) /* ty=Tensor[(256, 768), float32] */\n",
      "  %289 = nn.bias_add(%288, %hybridbertencoder2_transformer3_multiheadattentioncell0_value_bias, axis=-1) /* ty=Tensor[(256, 768), float32] */\n",
      "  %290 = reshape(%289, newshape=[1, 256, 768]) /* ty=Tensor[(1, 256, 768), float32] */\n",
      "  %291 = reshape(%290, newshape=[0, 0, 12, -1]) /* ty=Tensor[(1, 256, 12, 64), float32] */\n",
      "  %292 = transpose(%291, axes=[0, 2, 1, 3]) /* ty=Tensor[(1, 12, 256, 64), float32] */\n",
      "  %293 = _contrib_reverse_reshape(%292, newshape=[-1, 0, 0], reverse=True) /* ty=Tensor[(12, 256, 64), float32] */\n",
      "  %294 = transpose(%293, axes=[0, 2, 1]) /* ty=Tensor[(12, 64, 256), float32] */\n",
      "  %295 = nn.batch_matmul(%286, %294) /* ty=Tensor[(12, 256, 64), float32] */\n",
      "  %296 = _contrib_reverse_reshape(%295, newshape=[-1, 12, 0, 0], reverse=True) /* ty=Tensor[(1, 12, 256, 64), float32] */\n",
      "  %297 = transpose(%296, axes=[0, 2, 1, 3]) /* ty=Tensor[(1, 256, 12, 64), float32] */\n",
      "  %298 = reshape(%297, newshape=[0, 0, -1]) /* ty=Tensor[(1, 256, 768), float32] */\n",
      "  %299 = _contrib_reverse_reshape(%298, newshape=[-1, 0], reverse=True) /* ty=Tensor[(256, 768), float32] */\n",
      "  %300 = nn.dense(%299, %hybridbertencoder2_transformer3_proj_weight, units=768) /* ty=Tensor[(256, 768), float32] */\n",
      "  %301 = nn.bias_add(%300, %hybridbertencoder2_transformer3_proj_bias, axis=-1) /* ty=Tensor[(256, 768), float32] */\n",
      "  %302 = reshape(%301, newshape=[1, 256, 768]) /* ty=Tensor[(1, 256, 768), float32] */\n",
      "  %303 = nn.dropout(%302, rate=0.1) /* ty=(Tensor[(1, 256, 768), float32], Tensor[(1, 256, 768), float32]) */\n",
      "  %304 = %303.0\n",
      "  %305 = add(%304, %257) /* ty=Tensor[(1, 256, 768), float32] */\n",
      "  %306 = nn.layer_norm(%305, %hybridbertencoder2_transformer3_bertlayernorm0_gamma, %hybridbertencoder2_transformer3_bertlayernorm0_beta, epsilon=1e-12) /* ty=Tensor[(1, 256, 768), float32] */\n",
      "  %307 = _contrib_reverse_reshape(%306, newshape=[-1, 0], reverse=True) /* ty=Tensor[(256, 768), float32] */\n",
      "  %308 = nn.dense(%307, %hybridbertencoder2_transformer3_bertpositionwiseffn0_ffn_1_weight, units=3072) /* ty=Tensor[(256, 3072), float32] */\n",
      "  %309 = nn.bias_add(%308, %hybridbertencoder2_transformer3_bertpositionwiseffn0_ffn_1_bias, axis=-1) /* ty=Tensor[(256, 3072), float32] */\n",
      "  %310 = reshape(%309, newshape=[1, 256, 3072]) /* ty=Tensor[(1, 256, 3072), float32] */\n",
      "  %311 = multiply(%310, 0.5f /* ty=float32 */) /* ty=Tensor[(1, 256, 3072), float32] */\n",
      "  %312 = divide(%310, 1.41421f /* ty=float32 */) /* ty=Tensor[(1, 256, 3072), float32] */\n",
      "  %313 = erf(%312) /* ty=Tensor[(1, 256, 3072), float32] */\n",
      "  %314 = add(%313, 1f /* ty=float32 */) /* ty=Tensor[(1, 256, 3072), float32] */\n",
      "  %315 = multiply(%311, %314) /* ty=Tensor[(1, 256, 3072), float32] */\n",
      "  %316 = _contrib_reverse_reshape(%315, newshape=[-1, 0], reverse=True) /* ty=Tensor[(256, 3072), float32] */\n",
      "  %317 = nn.dense(%316, %hybridbertencoder2_transformer3_bertpositionwiseffn0_ffn_2_weight, units=768) /* ty=Tensor[(256, 768), float32] */\n",
      "  %318 = nn.bias_add(%317, %hybridbertencoder2_transformer3_bertpositionwiseffn0_ffn_2_bias, axis=-1) /* ty=Tensor[(256, 768), float32] */\n",
      "  %319 = reshape(%318, newshape=[1, 256, 768]) /* ty=Tensor[(1, 256, 768), float32] */\n",
      "  %320 = nn.dropout(%319, rate=0.1) /* ty=(Tensor[(1, 256, 768), float32], Tensor[(1, 256, 768), float32]) */\n",
      "  %321 = %320.0\n",
      "  %322 = add(%321, %306) /* ty=Tensor[(1, 256, 768), float32] */\n",
      "  %323 = nn.layer_norm(%322, %hybridbertencoder2_transformer3_bertpositionwiseffn0_bertlayernorm0_gamma, %hybridbertencoder2_transformer3_bertpositionwiseffn0_bertlayernorm0_beta, epsilon=1e-12) /* ty=Tensor[(1, 256, 768), float32] */\n",
      "  %324 = _contrib_reverse_reshape(%323, newshape=[-1, 0], reverse=True) /* ty=Tensor[(256, 768), float32] */\n",
      "  %325 = nn.dense(%324, %hybridbertencoder2_transformer4_multiheadattentioncell0_query_weight, units=768) /* ty=Tensor[(256, 768), float32] */\n",
      "  %326 = nn.bias_add(%325, %hybridbertencoder2_transformer4_multiheadattentioncell0_query_bias, axis=-1) /* ty=Tensor[(256, 768), float32] */\n",
      "  %327 = reshape(%326, newshape=[1, 256, 768]) /* ty=Tensor[(1, 256, 768), float32] */\n",
      "  %328 = reshape(%327, newshape=[0, 0, 12, -1]) /* ty=Tensor[(1, 256, 12, 64), float32] */\n",
      "  %329 = transpose(%328, axes=[0, 2, 1, 3]) /* ty=Tensor[(1, 12, 256, 64), float32] */\n",
      "  %330 = _contrib_reverse_reshape(%329, newshape=[-1, 0, 0], reverse=True) /* ty=Tensor[(12, 256, 64), float32] */\n",
      "  %331 = shape_of(%330, dtype=\"int32\") /* ty=Tensor[(3,), int32] */\n",
      "  %332 = take(%331, 2 /* ty=int32 */) /* ty=int32 */\n",
      "  %333 = cast(%332, dtype=\"float32\") /* ty=float32 */\n",
      "  %334 = sqrt(%333) /* ty=float32 */\n",
      "  %335 = divide(%330, %334) /* ty=Tensor[(12, 256, 64), float32] */\n",
      "  %336 = _contrib_reverse_reshape(%323, newshape=[-1, 0], reverse=True) /* ty=Tensor[(256, 768), float32] */\n",
      "  %337 = nn.dense(%336, %hybridbertencoder2_transformer4_multiheadattentioncell0_key_weight, units=768) /* ty=Tensor[(256, 768), float32] */\n",
      "  %338 = nn.bias_add(%337, %hybridbertencoder2_transformer4_multiheadattentioncell0_key_bias, axis=-1) /* ty=Tensor[(256, 768), float32] */\n",
      "  %339 = reshape(%338, newshape=[1, 256, 768]) /* ty=Tensor[(1, 256, 768), float32] */\n",
      "  %340 = reshape(%339, newshape=[0, 0, 12, -1]) /* ty=Tensor[(1, 256, 12, 64), float32] */\n",
      "  %341 = transpose(%340, axes=[0, 2, 1, 3]) /* ty=Tensor[(1, 12, 256, 64), float32] */\n",
      "  %342 = _contrib_reverse_reshape(%341, newshape=[-1, 0, 0], reverse=True) /* ty=Tensor[(12, 256, 64), float32] */\n",
      "  %343 = nn.batch_matmul(%335, %342) /* ty=Tensor[(12, 256, 256), float32] */\n",
      "  %344 = ones_like(%343) /* ty=Tensor[(12, 256, 256), float32] */\n",
      "  %345 = multiply(%344, -1e+18f /* ty=float32 */) /* ty=Tensor[(12, 256, 256), float32] */\n",
      "  %346 = where(%30, %343, %345) /* ty=Tensor[(12, 256, 256), float32] */\n",
      "  %347 = nn.softmax(%346) /* ty=Tensor[(12, 256, 256), float32] */\n",
      "  %348 = multiply(%347, %30) /* ty=Tensor[(12, 256, 256), float32] */\n",
      "  %349 = nn.dropout(%348, rate=0.1) /* ty=(Tensor[(12, 256, 256), float32], Tensor[(12, 256, 256), float32]) */\n",
      "  %350 = %349.0\n",
      "  %351 = _contrib_reverse_reshape(%350, newshape=[-1, 12, 0, 0], reverse=True) /* ty=Tensor[(1, 12, 256, 256), float32] */\n",
      "  %352 = _contrib_reverse_reshape(%351, newshape=[-1, 0, 0], reverse=True) /* ty=Tensor[(12, 256, 256), float32] */\n",
      "  %353 = _contrib_reverse_reshape(%323, newshape=[-1, 0], reverse=True) /* ty=Tensor[(256, 768), float32] */\n",
      "  %354 = nn.dense(%353, %hybridbertencoder2_transformer4_multiheadattentioncell0_value_weight, units=768) /* ty=Tensor[(256, 768), float32] */\n",
      "  %355 = nn.bias_add(%354, %hybridbertencoder2_transformer4_multiheadattentioncell0_value_bias, axis=-1) /* ty=Tensor[(256, 768), float32] */\n",
      "  %356 = reshape(%355, newshape=[1, 256, 768]) /* ty=Tensor[(1, 256, 768), float32] */\n",
      "  %357 = reshape(%356, newshape=[0, 0, 12, -1]) /* ty=Tensor[(1, 256, 12, 64), float32] */\n",
      "  %358 = transpose(%357, axes=[0, 2, 1, 3]) /* ty=Tensor[(1, 12, 256, 64), float32] */\n",
      "  %359 = _contrib_reverse_reshape(%358, newshape=[-1, 0, 0], reverse=True) /* ty=Tensor[(12, 256, 64), float32] */\n",
      "  %360 = transpose(%359, axes=[0, 2, 1]) /* ty=Tensor[(12, 64, 256), float32] */\n",
      "  %361 = nn.batch_matmul(%352, %360) /* ty=Tensor[(12, 256, 64), float32] */\n",
      "  %362 = _contrib_reverse_reshape(%361, newshape=[-1, 12, 0, 0], reverse=True) /* ty=Tensor[(1, 12, 256, 64), float32] */\n",
      "  %363 = transpose(%362, axes=[0, 2, 1, 3]) /* ty=Tensor[(1, 256, 12, 64), float32] */\n",
      "  %364 = reshape(%363, newshape=[0, 0, -1]) /* ty=Tensor[(1, 256, 768), float32] */\n",
      "  %365 = _contrib_reverse_reshape(%364, newshape=[-1, 0], reverse=True) /* ty=Tensor[(256, 768), float32] */\n",
      "  %366 = nn.dense(%365, %hybridbertencoder2_transformer4_proj_weight, units=768) /* ty=Tensor[(256, 768), float32] */\n",
      "  %367 = nn.bias_add(%366, %hybridbertencoder2_transformer4_proj_bias, axis=-1) /* ty=Tensor[(256, 768), float32] */\n",
      "  %368 = reshape(%367, newshape=[1, 256, 768]) /* ty=Tensor[(1, 256, 768), float32] */\n",
      "  %369 = nn.dropout(%368, rate=0.1) /* ty=(Tensor[(1, 256, 768), float32], Tensor[(1, 256, 768), float32]) */\n",
      "  %370 = %369.0\n",
      "  %371 = add(%370, %323) /* ty=Tensor[(1, 256, 768), float32] */\n",
      "  %372 = nn.layer_norm(%371, %hybridbertencoder2_transformer4_bertlayernorm0_gamma, %hybridbertencoder2_transformer4_bertlayernorm0_beta, epsilon=1e-12) /* ty=Tensor[(1, 256, 768), float32] */\n",
      "  %373 = _contrib_reverse_reshape(%372, newshape=[-1, 0], reverse=True) /* ty=Tensor[(256, 768), float32] */\n",
      "  %374 = nn.dense(%373, %hybridbertencoder2_transformer4_bertpositionwiseffn0_ffn_1_weight, units=3072) /* ty=Tensor[(256, 3072), float32] */\n",
      "  %375 = nn.bias_add(%374, %hybridbertencoder2_transformer4_bertpositionwiseffn0_ffn_1_bias, axis=-1) /* ty=Tensor[(256, 3072), float32] */\n",
      "  %376 = reshape(%375, newshape=[1, 256, 3072]) /* ty=Tensor[(1, 256, 3072), float32] */\n",
      "  %377 = multiply(%376, 0.5f /* ty=float32 */) /* ty=Tensor[(1, 256, 3072), float32] */\n",
      "  %378 = divide(%376, 1.41421f /* ty=float32 */) /* ty=Tensor[(1, 256, 3072), float32] */\n",
      "  %379 = erf(%378) /* ty=Tensor[(1, 256, 3072), float32] */\n",
      "  %380 = add(%379, 1f /* ty=float32 */) /* ty=Tensor[(1, 256, 3072), float32] */\n",
      "  %381 = multiply(%377, %380) /* ty=Tensor[(1, 256, 3072), float32] */\n",
      "  %382 = _contrib_reverse_reshape(%381, newshape=[-1, 0], reverse=True) /* ty=Tensor[(256, 3072), float32] */\n",
      "  %383 = nn.dense(%382, %hybridbertencoder2_transformer4_bertpositionwiseffn0_ffn_2_weight, units=768) /* ty=Tensor[(256, 768), float32] */\n",
      "  %384 = nn.bias_add(%383, %hybridbertencoder2_transformer4_bertpositionwiseffn0_ffn_2_bias, axis=-1) /* ty=Tensor[(256, 768), float32] */\n",
      "  %385 = reshape(%384, newshape=[1, 256, 768]) /* ty=Tensor[(1, 256, 768), float32] */\n",
      "  %386 = nn.dropout(%385, rate=0.1) /* ty=(Tensor[(1, 256, 768), float32], Tensor[(1, 256, 768), float32]) */\n",
      "  %387 = %386.0\n",
      "  %388 = add(%387, %372) /* ty=Tensor[(1, 256, 768), float32] */\n",
      "  %389 = nn.layer_norm(%388, %hybridbertencoder2_transformer4_bertpositionwiseffn0_bertlayernorm0_gamma, %hybridbertencoder2_transformer4_bertpositionwiseffn0_bertlayernorm0_beta, epsilon=1e-12) /* ty=Tensor[(1, 256, 768), float32] */\n",
      "  %390 = _contrib_reverse_reshape(%389, newshape=[-1, 0], reverse=True) /* ty=Tensor[(256, 768), float32] */\n",
      "  %391 = nn.dense(%390, %hybridbertencoder2_transformer5_multiheadattentioncell0_query_weight, units=768) /* ty=Tensor[(256, 768), float32] */\n",
      "  %392 = nn.bias_add(%391, %hybridbertencoder2_transformer5_multiheadattentioncell0_query_bias, axis=-1) /* ty=Tensor[(256, 768), float32] */\n",
      "  %393 = reshape(%392, newshape=[1, 256, 768]) /* ty=Tensor[(1, 256, 768), float32] */\n",
      "  %394 = reshape(%393, newshape=[0, 0, 12, -1]) /* ty=Tensor[(1, 256, 12, 64), float32] */\n",
      "  %395 = transpose(%394, axes=[0, 2, 1, 3]) /* ty=Tensor[(1, 12, 256, 64), float32] */\n",
      "  %396 = _contrib_reverse_reshape(%395, newshape=[-1, 0, 0], reverse=True) /* ty=Tensor[(12, 256, 64), float32] */\n",
      "  %397 = shape_of(%396, dtype=\"int32\") /* ty=Tensor[(3,), int32] */\n",
      "  %398 = take(%397, 2 /* ty=int32 */) /* ty=int32 */\n",
      "  %399 = cast(%398, dtype=\"float32\") /* ty=float32 */\n",
      "  %400 = sqrt(%399) /* ty=float32 */\n",
      "  %401 = divide(%396, %400) /* ty=Tensor[(12, 256, 64), float32] */\n",
      "  %402 = _contrib_reverse_reshape(%389, newshape=[-1, 0], reverse=True) /* ty=Tensor[(256, 768), float32] */\n",
      "  %403 = nn.dense(%402, %hybridbertencoder2_transformer5_multiheadattentioncell0_key_weight, units=768) /* ty=Tensor[(256, 768), float32] */\n",
      "  %404 = nn.bias_add(%403, %hybridbertencoder2_transformer5_multiheadattentioncell0_key_bias, axis=-1) /* ty=Tensor[(256, 768), float32] */\n",
      "  %405 = reshape(%404, newshape=[1, 256, 768]) /* ty=Tensor[(1, 256, 768), float32] */\n",
      "  %406 = reshape(%405, newshape=[0, 0, 12, -1]) /* ty=Tensor[(1, 256, 12, 64), float32] */\n",
      "  %407 = transpose(%406, axes=[0, 2, 1, 3]) /* ty=Tensor[(1, 12, 256, 64), float32] */\n",
      "  %408 = _contrib_reverse_reshape(%407, newshape=[-1, 0, 0], reverse=True) /* ty=Tensor[(12, 256, 64), float32] */\n",
      "  %409 = nn.batch_matmul(%401, %408) /* ty=Tensor[(12, 256, 256), float32] */\n",
      "  %410 = ones_like(%409) /* ty=Tensor[(12, 256, 256), float32] */\n",
      "  %411 = multiply(%410, -1e+18f /* ty=float32 */) /* ty=Tensor[(12, 256, 256), float32] */\n",
      "  %412 = where(%27, %409, %411) /* ty=Tensor[(12, 256, 256), float32] */\n",
      "  %413 = nn.softmax(%412) /* ty=Tensor[(12, 256, 256), float32] */\n",
      "  %414 = multiply(%413, %27) /* ty=Tensor[(12, 256, 256), float32] */\n",
      "  %415 = nn.dropout(%414, rate=0.1) /* ty=(Tensor[(12, 256, 256), float32], Tensor[(12, 256, 256), float32]) */\n",
      "  %416 = %415.0\n",
      "  %417 = _contrib_reverse_reshape(%416, newshape=[-1, 12, 0, 0], reverse=True) /* ty=Tensor[(1, 12, 256, 256), float32] */\n",
      "  %418 = _contrib_reverse_reshape(%417, newshape=[-1, 0, 0], reverse=True) /* ty=Tensor[(12, 256, 256), float32] */\n",
      "  %419 = _contrib_reverse_reshape(%389, newshape=[-1, 0], reverse=True) /* ty=Tensor[(256, 768), float32] */\n",
      "  %420 = nn.dense(%419, %hybridbertencoder2_transformer5_multiheadattentioncell0_value_weight, units=768) /* ty=Tensor[(256, 768), float32] */\n",
      "  %421 = nn.bias_add(%420, %hybridbertencoder2_transformer5_multiheadattentioncell0_value_bias, axis=-1) /* ty=Tensor[(256, 768), float32] */\n",
      "  %422 = reshape(%421, newshape=[1, 256, 768]) /* ty=Tensor[(1, 256, 768), float32] */\n",
      "  %423 = reshape(%422, newshape=[0, 0, 12, -1]) /* ty=Tensor[(1, 256, 12, 64), float32] */\n",
      "  %424 = transpose(%423, axes=[0, 2, 1, 3]) /* ty=Tensor[(1, 12, 256, 64), float32] */\n",
      "  %425 = _contrib_reverse_reshape(%424, newshape=[-1, 0, 0], reverse=True) /* ty=Tensor[(12, 256, 64), float32] */\n",
      "  %426 = transpose(%425, axes=[0, 2, 1]) /* ty=Tensor[(12, 64, 256), float32] */\n",
      "  %427 = nn.batch_matmul(%418, %426) /* ty=Tensor[(12, 256, 64), float32] */\n",
      "  %428 = _contrib_reverse_reshape(%427, newshape=[-1, 12, 0, 0], reverse=True) /* ty=Tensor[(1, 12, 256, 64), float32] */\n",
      "  %429 = transpose(%428, axes=[0, 2, 1, 3]) /* ty=Tensor[(1, 256, 12, 64), float32] */\n",
      "  %430 = reshape(%429, newshape=[0, 0, -1]) /* ty=Tensor[(1, 256, 768), float32] */\n",
      "  %431 = _contrib_reverse_reshape(%430, newshape=[-1, 0], reverse=True) /* ty=Tensor[(256, 768), float32] */\n",
      "  %432 = nn.dense(%431, %hybridbertencoder2_transformer5_proj_weight, units=768) /* ty=Tensor[(256, 768), float32] */\n",
      "  %433 = nn.bias_add(%432, %hybridbertencoder2_transformer5_proj_bias, axis=-1) /* ty=Tensor[(256, 768), float32] */\n",
      "  %434 = reshape(%433, newshape=[1, 256, 768]) /* ty=Tensor[(1, 256, 768), float32] */\n",
      "  %435 = nn.dropout(%434, rate=0.1) /* ty=(Tensor[(1, 256, 768), float32], Tensor[(1, 256, 768), float32]) */\n",
      "  %436 = %435.0\n",
      "  %437 = add(%436, %389) /* ty=Tensor[(1, 256, 768), float32] */\n",
      "  %438 = nn.layer_norm(%437, %hybridbertencoder2_transformer5_bertlayernorm0_gamma, %hybridbertencoder2_transformer5_bertlayernorm0_beta, epsilon=1e-12) /* ty=Tensor[(1, 256, 768), float32] */\n",
      "  %439 = _contrib_reverse_reshape(%438, newshape=[-1, 0], reverse=True) /* ty=Tensor[(256, 768), float32] */\n",
      "  %440 = nn.dense(%439, %hybridbertencoder2_transformer5_bertpositionwiseffn0_ffn_1_weight, units=3072) /* ty=Tensor[(256, 3072), float32] */\n",
      "  %441 = nn.bias_add(%440, %hybridbertencoder2_transformer5_bertpositionwiseffn0_ffn_1_bias, axis=-1) /* ty=Tensor[(256, 3072), float32] */\n",
      "  %442 = reshape(%441, newshape=[1, 256, 3072]) /* ty=Tensor[(1, 256, 3072), float32] */\n",
      "  %443 = multiply(%442, 0.5f /* ty=float32 */) /* ty=Tensor[(1, 256, 3072), float32] */\n",
      "  %444 = divide(%442, 1.41421f /* ty=float32 */) /* ty=Tensor[(1, 256, 3072), float32] */\n",
      "  %445 = erf(%444) /* ty=Tensor[(1, 256, 3072), float32] */\n",
      "  %446 = add(%445, 1f /* ty=float32 */) /* ty=Tensor[(1, 256, 3072), float32] */\n",
      "  %447 = multiply(%443, %446) /* ty=Tensor[(1, 256, 3072), float32] */\n",
      "  %448 = _contrib_reverse_reshape(%447, newshape=[-1, 0], reverse=True) /* ty=Tensor[(256, 3072), float32] */\n",
      "  %449 = nn.dense(%448, %hybridbertencoder2_transformer5_bertpositionwiseffn0_ffn_2_weight, units=768) /* ty=Tensor[(256, 768), float32] */\n",
      "  %450 = nn.bias_add(%449, %hybridbertencoder2_transformer5_bertpositionwiseffn0_ffn_2_bias, axis=-1) /* ty=Tensor[(256, 768), float32] */\n",
      "  %451 = reshape(%450, newshape=[1, 256, 768]) /* ty=Tensor[(1, 256, 768), float32] */\n",
      "  %452 = nn.dropout(%451, rate=0.1) /* ty=(Tensor[(1, 256, 768), float32], Tensor[(1, 256, 768), float32]) */\n",
      "  %453 = %452.0\n",
      "  %454 = add(%453, %438) /* ty=Tensor[(1, 256, 768), float32] */\n",
      "  %455 = nn.layer_norm(%454, %hybridbertencoder2_transformer5_bertpositionwiseffn0_bertlayernorm0_gamma, %hybridbertencoder2_transformer5_bertpositionwiseffn0_bertlayernorm0_beta, epsilon=1e-12) /* ty=Tensor[(1, 256, 768), float32] */\n",
      "  %456 = _contrib_reverse_reshape(%455, newshape=[-1, 0], reverse=True) /* ty=Tensor[(256, 768), float32] */\n",
      "  %457 = nn.dense(%456, %hybridbertencoder2_transformer6_multiheadattentioncell0_query_weight, units=768) /* ty=Tensor[(256, 768), float32] */\n",
      "  %458 = nn.bias_add(%457, %hybridbertencoder2_transformer6_multiheadattentioncell0_query_bias, axis=-1) /* ty=Tensor[(256, 768), float32] */\n",
      "  %459 = reshape(%458, newshape=[1, 256, 768]) /* ty=Tensor[(1, 256, 768), float32] */\n",
      "  %460 = reshape(%459, newshape=[0, 0, 12, -1]) /* ty=Tensor[(1, 256, 12, 64), float32] */\n",
      "  %461 = transpose(%460, axes=[0, 2, 1, 3]) /* ty=Tensor[(1, 12, 256, 64), float32] */\n",
      "  %462 = _contrib_reverse_reshape(%461, newshape=[-1, 0, 0], reverse=True) /* ty=Tensor[(12, 256, 64), float32] */\n",
      "  %463 = shape_of(%462, dtype=\"int32\") /* ty=Tensor[(3,), int32] */\n",
      "  %464 = take(%463, 2 /* ty=int32 */) /* ty=int32 */\n",
      "  %465 = cast(%464, dtype=\"float32\") /* ty=float32 */\n",
      "  %466 = sqrt(%465) /* ty=float32 */\n",
      "  %467 = divide(%462, %466) /* ty=Tensor[(12, 256, 64), float32] */\n",
      "  %468 = _contrib_reverse_reshape(%455, newshape=[-1, 0], reverse=True) /* ty=Tensor[(256, 768), float32] */\n",
      "  %469 = nn.dense(%468, %hybridbertencoder2_transformer6_multiheadattentioncell0_key_weight, units=768) /* ty=Tensor[(256, 768), float32] */\n",
      "  %470 = nn.bias_add(%469, %hybridbertencoder2_transformer6_multiheadattentioncell0_key_bias, axis=-1) /* ty=Tensor[(256, 768), float32] */\n",
      "  %471 = reshape(%470, newshape=[1, 256, 768]) /* ty=Tensor[(1, 256, 768), float32] */\n",
      "  %472 = reshape(%471, newshape=[0, 0, 12, -1]) /* ty=Tensor[(1, 256, 12, 64), float32] */\n",
      "  %473 = transpose(%472, axes=[0, 2, 1, 3]) /* ty=Tensor[(1, 12, 256, 64), float32] */\n",
      "  %474 = _contrib_reverse_reshape(%473, newshape=[-1, 0, 0], reverse=True) /* ty=Tensor[(12, 256, 64), float32] */\n",
      "  %475 = nn.batch_matmul(%467, %474) /* ty=Tensor[(12, 256, 256), float32] */\n",
      "  %476 = ones_like(%475) /* ty=Tensor[(12, 256, 256), float32] */\n",
      "  %477 = multiply(%476, -1e+18f /* ty=float32 */) /* ty=Tensor[(12, 256, 256), float32] */\n",
      "  %478 = where(%24, %475, %477) /* ty=Tensor[(12, 256, 256), float32] */\n",
      "  %479 = nn.softmax(%478) /* ty=Tensor[(12, 256, 256), float32] */\n",
      "  %480 = multiply(%479, %24) /* ty=Tensor[(12, 256, 256), float32] */\n",
      "  %481 = nn.dropout(%480, rate=0.1) /* ty=(Tensor[(12, 256, 256), float32], Tensor[(12, 256, 256), float32]) */\n",
      "  %482 = %481.0\n",
      "  %483 = _contrib_reverse_reshape(%482, newshape=[-1, 12, 0, 0], reverse=True) /* ty=Tensor[(1, 12, 256, 256), float32] */\n",
      "  %484 = _contrib_reverse_reshape(%483, newshape=[-1, 0, 0], reverse=True) /* ty=Tensor[(12, 256, 256), float32] */\n",
      "  %485 = _contrib_reverse_reshape(%455, newshape=[-1, 0], reverse=True) /* ty=Tensor[(256, 768), float32] */\n",
      "  %486 = nn.dense(%485, %hybridbertencoder2_transformer6_multiheadattentioncell0_value_weight, units=768) /* ty=Tensor[(256, 768), float32] */\n",
      "  %487 = nn.bias_add(%486, %hybridbertencoder2_transformer6_multiheadattentioncell0_value_bias, axis=-1) /* ty=Tensor[(256, 768), float32] */\n",
      "  %488 = reshape(%487, newshape=[1, 256, 768]) /* ty=Tensor[(1, 256, 768), float32] */\n",
      "  %489 = reshape(%488, newshape=[0, 0, 12, -1]) /* ty=Tensor[(1, 256, 12, 64), float32] */\n",
      "  %490 = transpose(%489, axes=[0, 2, 1, 3]) /* ty=Tensor[(1, 12, 256, 64), float32] */\n",
      "  %491 = _contrib_reverse_reshape(%490, newshape=[-1, 0, 0], reverse=True) /* ty=Tensor[(12, 256, 64), float32] */\n",
      "  %492 = transpose(%491, axes=[0, 2, 1]) /* ty=Tensor[(12, 64, 256), float32] */\n",
      "  %493 = nn.batch_matmul(%484, %492) /* ty=Tensor[(12, 256, 64), float32] */\n",
      "  %494 = _contrib_reverse_reshape(%493, newshape=[-1, 12, 0, 0], reverse=True) /* ty=Tensor[(1, 12, 256, 64), float32] */\n",
      "  %495 = transpose(%494, axes=[0, 2, 1, 3]) /* ty=Tensor[(1, 256, 12, 64), float32] */\n",
      "  %496 = reshape(%495, newshape=[0, 0, -1]) /* ty=Tensor[(1, 256, 768), float32] */\n",
      "  %497 = _contrib_reverse_reshape(%496, newshape=[-1, 0], reverse=True) /* ty=Tensor[(256, 768), float32] */\n",
      "  %498 = nn.dense(%497, %hybridbertencoder2_transformer6_proj_weight, units=768) /* ty=Tensor[(256, 768), float32] */\n",
      "  %499 = nn.bias_add(%498, %hybridbertencoder2_transformer6_proj_bias, axis=-1) /* ty=Tensor[(256, 768), float32] */\n",
      "  %500 = reshape(%499, newshape=[1, 256, 768]) /* ty=Tensor[(1, 256, 768), float32] */\n",
      "  %501 = nn.dropout(%500, rate=0.1) /* ty=(Tensor[(1, 256, 768), float32], Tensor[(1, 256, 768), float32]) */\n",
      "  %502 = %501.0\n",
      "  %503 = add(%502, %455) /* ty=Tensor[(1, 256, 768), float32] */\n",
      "  %504 = nn.layer_norm(%503, %hybridbertencoder2_transformer6_bertlayernorm0_gamma, %hybridbertencoder2_transformer6_bertlayernorm0_beta, epsilon=1e-12) /* ty=Tensor[(1, 256, 768), float32] */\n",
      "  %505 = _contrib_reverse_reshape(%504, newshape=[-1, 0], reverse=True) /* ty=Tensor[(256, 768), float32] */\n",
      "  %506 = nn.dense(%505, %hybridbertencoder2_transformer6_bertpositionwiseffn0_ffn_1_weight, units=3072) /* ty=Tensor[(256, 3072), float32] */\n",
      "  %507 = nn.bias_add(%506, %hybridbertencoder2_transformer6_bertpositionwiseffn0_ffn_1_bias, axis=-1) /* ty=Tensor[(256, 3072), float32] */\n",
      "  %508 = reshape(%507, newshape=[1, 256, 3072]) /* ty=Tensor[(1, 256, 3072), float32] */\n",
      "  %509 = multiply(%508, 0.5f /* ty=float32 */) /* ty=Tensor[(1, 256, 3072), float32] */\n",
      "  %510 = divide(%508, 1.41421f /* ty=float32 */) /* ty=Tensor[(1, 256, 3072), float32] */\n",
      "  %511 = erf(%510) /* ty=Tensor[(1, 256, 3072), float32] */\n",
      "  %512 = add(%511, 1f /* ty=float32 */) /* ty=Tensor[(1, 256, 3072), float32] */\n",
      "  %513 = multiply(%509, %512) /* ty=Tensor[(1, 256, 3072), float32] */\n",
      "  %514 = _contrib_reverse_reshape(%513, newshape=[-1, 0], reverse=True) /* ty=Tensor[(256, 3072), float32] */\n",
      "  %515 = nn.dense(%514, %hybridbertencoder2_transformer6_bertpositionwiseffn0_ffn_2_weight, units=768) /* ty=Tensor[(256, 768), float32] */\n",
      "  %516 = nn.bias_add(%515, %hybridbertencoder2_transformer6_bertpositionwiseffn0_ffn_2_bias, axis=-1) /* ty=Tensor[(256, 768), float32] */\n",
      "  %517 = reshape(%516, newshape=[1, 256, 768]) /* ty=Tensor[(1, 256, 768), float32] */\n",
      "  %518 = nn.dropout(%517, rate=0.1) /* ty=(Tensor[(1, 256, 768), float32], Tensor[(1, 256, 768), float32]) */\n",
      "  %519 = %518.0\n",
      "  %520 = add(%519, %504) /* ty=Tensor[(1, 256, 768), float32] */\n",
      "  %521 = nn.layer_norm(%520, %hybridbertencoder2_transformer6_bertpositionwiseffn0_bertlayernorm0_gamma, %hybridbertencoder2_transformer6_bertpositionwiseffn0_bertlayernorm0_beta, epsilon=1e-12) /* ty=Tensor[(1, 256, 768), float32] */\n",
      "  %522 = _contrib_reverse_reshape(%521, newshape=[-1, 0], reverse=True) /* ty=Tensor[(256, 768), float32] */\n",
      "  %523 = nn.dense(%522, %hybridbertencoder2_transformer7_multiheadattentioncell0_query_weight, units=768) /* ty=Tensor[(256, 768), float32] */\n",
      "  %524 = nn.bias_add(%523, %hybridbertencoder2_transformer7_multiheadattentioncell0_query_bias, axis=-1) /* ty=Tensor[(256, 768), float32] */\n",
      "  %525 = reshape(%524, newshape=[1, 256, 768]) /* ty=Tensor[(1, 256, 768), float32] */\n",
      "  %526 = reshape(%525, newshape=[0, 0, 12, -1]) /* ty=Tensor[(1, 256, 12, 64), float32] */\n",
      "  %527 = transpose(%526, axes=[0, 2, 1, 3]) /* ty=Tensor[(1, 12, 256, 64), float32] */\n",
      "  %528 = _contrib_reverse_reshape(%527, newshape=[-1, 0, 0], reverse=True) /* ty=Tensor[(12, 256, 64), float32] */\n",
      "  %529 = shape_of(%528, dtype=\"int32\") /* ty=Tensor[(3,), int32] */\n",
      "  %530 = take(%529, 2 /* ty=int32 */) /* ty=int32 */\n",
      "  %531 = cast(%530, dtype=\"float32\") /* ty=float32 */\n",
      "  %532 = sqrt(%531) /* ty=float32 */\n",
      "  %533 = divide(%528, %532) /* ty=Tensor[(12, 256, 64), float32] */\n",
      "  %534 = _contrib_reverse_reshape(%521, newshape=[-1, 0], reverse=True) /* ty=Tensor[(256, 768), float32] */\n",
      "  %535 = nn.dense(%534, %hybridbertencoder2_transformer7_multiheadattentioncell0_key_weight, units=768) /* ty=Tensor[(256, 768), float32] */\n",
      "  %536 = nn.bias_add(%535, %hybridbertencoder2_transformer7_multiheadattentioncell0_key_bias, axis=-1) /* ty=Tensor[(256, 768), float32] */\n",
      "  %537 = reshape(%536, newshape=[1, 256, 768]) /* ty=Tensor[(1, 256, 768), float32] */\n",
      "  %538 = reshape(%537, newshape=[0, 0, 12, -1]) /* ty=Tensor[(1, 256, 12, 64), float32] */\n",
      "  %539 = transpose(%538, axes=[0, 2, 1, 3]) /* ty=Tensor[(1, 12, 256, 64), float32] */\n",
      "  %540 = _contrib_reverse_reshape(%539, newshape=[-1, 0, 0], reverse=True) /* ty=Tensor[(12, 256, 64), float32] */\n",
      "  %541 = nn.batch_matmul(%533, %540) /* ty=Tensor[(12, 256, 256), float32] */\n",
      "  %542 = ones_like(%541) /* ty=Tensor[(12, 256, 256), float32] */\n",
      "  %543 = multiply(%542, -1e+18f /* ty=float32 */) /* ty=Tensor[(12, 256, 256), float32] */\n",
      "  %544 = where(%21, %541, %543) /* ty=Tensor[(12, 256, 256), float32] */\n",
      "  %545 = nn.softmax(%544) /* ty=Tensor[(12, 256, 256), float32] */\n",
      "  %546 = multiply(%545, %21) /* ty=Tensor[(12, 256, 256), float32] */\n",
      "  %547 = nn.dropout(%546, rate=0.1) /* ty=(Tensor[(12, 256, 256), float32], Tensor[(12, 256, 256), float32]) */\n",
      "  %548 = %547.0\n",
      "  %549 = _contrib_reverse_reshape(%548, newshape=[-1, 12, 0, 0], reverse=True) /* ty=Tensor[(1, 12, 256, 256), float32] */\n",
      "  %550 = _contrib_reverse_reshape(%549, newshape=[-1, 0, 0], reverse=True) /* ty=Tensor[(12, 256, 256), float32] */\n",
      "  %551 = _contrib_reverse_reshape(%521, newshape=[-1, 0], reverse=True) /* ty=Tensor[(256, 768), float32] */\n",
      "  %552 = nn.dense(%551, %hybridbertencoder2_transformer7_multiheadattentioncell0_value_weight, units=768) /* ty=Tensor[(256, 768), float32] */\n",
      "  %553 = nn.bias_add(%552, %hybridbertencoder2_transformer7_multiheadattentioncell0_value_bias, axis=-1) /* ty=Tensor[(256, 768), float32] */\n",
      "  %554 = reshape(%553, newshape=[1, 256, 768]) /* ty=Tensor[(1, 256, 768), float32] */\n",
      "  %555 = reshape(%554, newshape=[0, 0, 12, -1]) /* ty=Tensor[(1, 256, 12, 64), float32] */\n",
      "  %556 = transpose(%555, axes=[0, 2, 1, 3]) /* ty=Tensor[(1, 12, 256, 64), float32] */\n",
      "  %557 = _contrib_reverse_reshape(%556, newshape=[-1, 0, 0], reverse=True) /* ty=Tensor[(12, 256, 64), float32] */\n",
      "  %558 = transpose(%557, axes=[0, 2, 1]) /* ty=Tensor[(12, 64, 256), float32] */\n",
      "  %559 = nn.batch_matmul(%550, %558) /* ty=Tensor[(12, 256, 64), float32] */\n",
      "  %560 = _contrib_reverse_reshape(%559, newshape=[-1, 12, 0, 0], reverse=True) /* ty=Tensor[(1, 12, 256, 64), float32] */\n",
      "  %561 = transpose(%560, axes=[0, 2, 1, 3]) /* ty=Tensor[(1, 256, 12, 64), float32] */\n",
      "  %562 = reshape(%561, newshape=[0, 0, -1]) /* ty=Tensor[(1, 256, 768), float32] */\n",
      "  %563 = _contrib_reverse_reshape(%562, newshape=[-1, 0], reverse=True) /* ty=Tensor[(256, 768), float32] */\n",
      "  %564 = nn.dense(%563, %hybridbertencoder2_transformer7_proj_weight, units=768) /* ty=Tensor[(256, 768), float32] */\n",
      "  %565 = nn.bias_add(%564, %hybridbertencoder2_transformer7_proj_bias, axis=-1) /* ty=Tensor[(256, 768), float32] */\n",
      "  %566 = reshape(%565, newshape=[1, 256, 768]) /* ty=Tensor[(1, 256, 768), float32] */\n",
      "  %567 = nn.dropout(%566, rate=0.1) /* ty=(Tensor[(1, 256, 768), float32], Tensor[(1, 256, 768), float32]) */\n",
      "  %568 = %567.0\n",
      "  %569 = add(%568, %521) /* ty=Tensor[(1, 256, 768), float32] */\n",
      "  %570 = nn.layer_norm(%569, %hybridbertencoder2_transformer7_bertlayernorm0_gamma, %hybridbertencoder2_transformer7_bertlayernorm0_beta, epsilon=1e-12) /* ty=Tensor[(1, 256, 768), float32] */\n",
      "  %571 = _contrib_reverse_reshape(%570, newshape=[-1, 0], reverse=True) /* ty=Tensor[(256, 768), float32] */\n",
      "  %572 = nn.dense(%571, %hybridbertencoder2_transformer7_bertpositionwiseffn0_ffn_1_weight, units=3072) /* ty=Tensor[(256, 3072), float32] */\n",
      "  %573 = nn.bias_add(%572, %hybridbertencoder2_transformer7_bertpositionwiseffn0_ffn_1_bias, axis=-1) /* ty=Tensor[(256, 3072), float32] */\n",
      "  %574 = reshape(%573, newshape=[1, 256, 3072]) /* ty=Tensor[(1, 256, 3072), float32] */\n",
      "  %575 = multiply(%574, 0.5f /* ty=float32 */) /* ty=Tensor[(1, 256, 3072), float32] */\n",
      "  %576 = divide(%574, 1.41421f /* ty=float32 */) /* ty=Tensor[(1, 256, 3072), float32] */\n",
      "  %577 = erf(%576) /* ty=Tensor[(1, 256, 3072), float32] */\n",
      "  %578 = add(%577, 1f /* ty=float32 */) /* ty=Tensor[(1, 256, 3072), float32] */\n",
      "  %579 = multiply(%575, %578) /* ty=Tensor[(1, 256, 3072), float32] */\n",
      "  %580 = _contrib_reverse_reshape(%579, newshape=[-1, 0], reverse=True) /* ty=Tensor[(256, 3072), float32] */\n",
      "  %581 = nn.dense(%580, %hybridbertencoder2_transformer7_bertpositionwiseffn0_ffn_2_weight, units=768) /* ty=Tensor[(256, 768), float32] */\n",
      "  %582 = nn.bias_add(%581, %hybridbertencoder2_transformer7_bertpositionwiseffn0_ffn_2_bias, axis=-1) /* ty=Tensor[(256, 768), float32] */\n",
      "  %583 = reshape(%582, newshape=[1, 256, 768]) /* ty=Tensor[(1, 256, 768), float32] */\n",
      "  %584 = nn.dropout(%583, rate=0.1) /* ty=(Tensor[(1, 256, 768), float32], Tensor[(1, 256, 768), float32]) */\n",
      "  %585 = %584.0\n",
      "  %586 = add(%585, %570) /* ty=Tensor[(1, 256, 768), float32] */\n",
      "  %587 = nn.layer_norm(%586, %hybridbertencoder2_transformer7_bertpositionwiseffn0_bertlayernorm0_gamma, %hybridbertencoder2_transformer7_bertpositionwiseffn0_bertlayernorm0_beta, epsilon=1e-12) /* ty=Tensor[(1, 256, 768), float32] */\n",
      "  %588 = _contrib_reverse_reshape(%587, newshape=[-1, 0], reverse=True) /* ty=Tensor[(256, 768), float32] */\n",
      "  %589 = nn.dense(%588, %hybridbertencoder2_transformer8_multiheadattentioncell0_query_weight, units=768) /* ty=Tensor[(256, 768), float32] */\n",
      "  %590 = nn.bias_add(%589, %hybridbertencoder2_transformer8_multiheadattentioncell0_query_bias, axis=-1) /* ty=Tensor[(256, 768), float32] */\n",
      "  %591 = reshape(%590, newshape=[1, 256, 768]) /* ty=Tensor[(1, 256, 768), float32] */\n",
      "  %592 = reshape(%591, newshape=[0, 0, 12, -1]) /* ty=Tensor[(1, 256, 12, 64), float32] */\n",
      "  %593 = transpose(%592, axes=[0, 2, 1, 3]) /* ty=Tensor[(1, 12, 256, 64), float32] */\n",
      "  %594 = _contrib_reverse_reshape(%593, newshape=[-1, 0, 0], reverse=True) /* ty=Tensor[(12, 256, 64), float32] */\n",
      "  %595 = shape_of(%594, dtype=\"int32\") /* ty=Tensor[(3,), int32] */\n",
      "  %596 = take(%595, 2 /* ty=int32 */) /* ty=int32 */\n",
      "  %597 = cast(%596, dtype=\"float32\") /* ty=float32 */\n",
      "  %598 = sqrt(%597) /* ty=float32 */\n",
      "  %599 = divide(%594, %598) /* ty=Tensor[(12, 256, 64), float32] */\n",
      "  %600 = _contrib_reverse_reshape(%587, newshape=[-1, 0], reverse=True) /* ty=Tensor[(256, 768), float32] */\n",
      "  %601 = nn.dense(%600, %hybridbertencoder2_transformer8_multiheadattentioncell0_key_weight, units=768) /* ty=Tensor[(256, 768), float32] */\n",
      "  %602 = nn.bias_add(%601, %hybridbertencoder2_transformer8_multiheadattentioncell0_key_bias, axis=-1) /* ty=Tensor[(256, 768), float32] */\n",
      "  %603 = reshape(%602, newshape=[1, 256, 768]) /* ty=Tensor[(1, 256, 768), float32] */\n",
      "  %604 = reshape(%603, newshape=[0, 0, 12, -1]) /* ty=Tensor[(1, 256, 12, 64), float32] */\n",
      "  %605 = transpose(%604, axes=[0, 2, 1, 3]) /* ty=Tensor[(1, 12, 256, 64), float32] */\n",
      "  %606 = _contrib_reverse_reshape(%605, newshape=[-1, 0, 0], reverse=True) /* ty=Tensor[(12, 256, 64), float32] */\n",
      "  %607 = nn.batch_matmul(%599, %606) /* ty=Tensor[(12, 256, 256), float32] */\n",
      "  %608 = ones_like(%607) /* ty=Tensor[(12, 256, 256), float32] */\n",
      "  %609 = multiply(%608, -1e+18f /* ty=float32 */) /* ty=Tensor[(12, 256, 256), float32] */\n",
      "  %610 = where(%18, %607, %609) /* ty=Tensor[(12, 256, 256), float32] */\n",
      "  %611 = nn.softmax(%610) /* ty=Tensor[(12, 256, 256), float32] */\n",
      "  %612 = multiply(%611, %18) /* ty=Tensor[(12, 256, 256), float32] */\n",
      "  %613 = nn.dropout(%612, rate=0.1) /* ty=(Tensor[(12, 256, 256), float32], Tensor[(12, 256, 256), float32]) */\n",
      "  %614 = %613.0\n",
      "  %615 = _contrib_reverse_reshape(%614, newshape=[-1, 12, 0, 0], reverse=True) /* ty=Tensor[(1, 12, 256, 256), float32] */\n",
      "  %616 = _contrib_reverse_reshape(%615, newshape=[-1, 0, 0], reverse=True) /* ty=Tensor[(12, 256, 256), float32] */\n",
      "  %617 = _contrib_reverse_reshape(%587, newshape=[-1, 0], reverse=True) /* ty=Tensor[(256, 768), float32] */\n",
      "  %618 = nn.dense(%617, %hybridbertencoder2_transformer8_multiheadattentioncell0_value_weight, units=768) /* ty=Tensor[(256, 768), float32] */\n",
      "  %619 = nn.bias_add(%618, %hybridbertencoder2_transformer8_multiheadattentioncell0_value_bias, axis=-1) /* ty=Tensor[(256, 768), float32] */\n",
      "  %620 = reshape(%619, newshape=[1, 256, 768]) /* ty=Tensor[(1, 256, 768), float32] */\n",
      "  %621 = reshape(%620, newshape=[0, 0, 12, -1]) /* ty=Tensor[(1, 256, 12, 64), float32] */\n",
      "  %622 = transpose(%621, axes=[0, 2, 1, 3]) /* ty=Tensor[(1, 12, 256, 64), float32] */\n",
      "  %623 = _contrib_reverse_reshape(%622, newshape=[-1, 0, 0], reverse=True) /* ty=Tensor[(12, 256, 64), float32] */\n",
      "  %624 = transpose(%623, axes=[0, 2, 1]) /* ty=Tensor[(12, 64, 256), float32] */\n",
      "  %625 = nn.batch_matmul(%616, %624) /* ty=Tensor[(12, 256, 64), float32] */\n",
      "  %626 = _contrib_reverse_reshape(%625, newshape=[-1, 12, 0, 0], reverse=True) /* ty=Tensor[(1, 12, 256, 64), float32] */\n",
      "  %627 = transpose(%626, axes=[0, 2, 1, 3]) /* ty=Tensor[(1, 256, 12, 64), float32] */\n",
      "  %628 = reshape(%627, newshape=[0, 0, -1]) /* ty=Tensor[(1, 256, 768), float32] */\n",
      "  %629 = _contrib_reverse_reshape(%628, newshape=[-1, 0], reverse=True) /* ty=Tensor[(256, 768), float32] */\n",
      "  %630 = nn.dense(%629, %hybridbertencoder2_transformer8_proj_weight, units=768) /* ty=Tensor[(256, 768), float32] */\n",
      "  %631 = nn.bias_add(%630, %hybridbertencoder2_transformer8_proj_bias, axis=-1) /* ty=Tensor[(256, 768), float32] */\n",
      "  %632 = reshape(%631, newshape=[1, 256, 768]) /* ty=Tensor[(1, 256, 768), float32] */\n",
      "  %633 = nn.dropout(%632, rate=0.1) /* ty=(Tensor[(1, 256, 768), float32], Tensor[(1, 256, 768), float32]) */\n",
      "  %634 = %633.0\n",
      "  %635 = add(%634, %587) /* ty=Tensor[(1, 256, 768), float32] */\n",
      "  %636 = nn.layer_norm(%635, %hybridbertencoder2_transformer8_bertlayernorm0_gamma, %hybridbertencoder2_transformer8_bertlayernorm0_beta, epsilon=1e-12) /* ty=Tensor[(1, 256, 768), float32] */\n",
      "  %637 = _contrib_reverse_reshape(%636, newshape=[-1, 0], reverse=True) /* ty=Tensor[(256, 768), float32] */\n",
      "  %638 = nn.dense(%637, %hybridbertencoder2_transformer8_bertpositionwiseffn0_ffn_1_weight, units=3072) /* ty=Tensor[(256, 3072), float32] */\n",
      "  %639 = nn.bias_add(%638, %hybridbertencoder2_transformer8_bertpositionwiseffn0_ffn_1_bias, axis=-1) /* ty=Tensor[(256, 3072), float32] */\n",
      "  %640 = reshape(%639, newshape=[1, 256, 3072]) /* ty=Tensor[(1, 256, 3072), float32] */\n",
      "  %641 = multiply(%640, 0.5f /* ty=float32 */) /* ty=Tensor[(1, 256, 3072), float32] */\n",
      "  %642 = divide(%640, 1.41421f /* ty=float32 */) /* ty=Tensor[(1, 256, 3072), float32] */\n",
      "  %643 = erf(%642) /* ty=Tensor[(1, 256, 3072), float32] */\n",
      "  %644 = add(%643, 1f /* ty=float32 */) /* ty=Tensor[(1, 256, 3072), float32] */\n",
      "  %645 = multiply(%641, %644) /* ty=Tensor[(1, 256, 3072), float32] */\n",
      "  %646 = _contrib_reverse_reshape(%645, newshape=[-1, 0], reverse=True) /* ty=Tensor[(256, 3072), float32] */\n",
      "  %647 = nn.dense(%646, %hybridbertencoder2_transformer8_bertpositionwiseffn0_ffn_2_weight, units=768) /* ty=Tensor[(256, 768), float32] */\n",
      "  %648 = nn.bias_add(%647, %hybridbertencoder2_transformer8_bertpositionwiseffn0_ffn_2_bias, axis=-1) /* ty=Tensor[(256, 768), float32] */\n",
      "  %649 = reshape(%648, newshape=[1, 256, 768]) /* ty=Tensor[(1, 256, 768), float32] */\n",
      "  %650 = nn.dropout(%649, rate=0.1) /* ty=(Tensor[(1, 256, 768), float32], Tensor[(1, 256, 768), float32]) */\n",
      "  %651 = %650.0\n",
      "  %652 = add(%651, %636) /* ty=Tensor[(1, 256, 768), float32] */\n",
      "  %653 = nn.layer_norm(%652, %hybridbertencoder2_transformer8_bertpositionwiseffn0_bertlayernorm0_gamma, %hybridbertencoder2_transformer8_bertpositionwiseffn0_bertlayernorm0_beta, epsilon=1e-12) /* ty=Tensor[(1, 256, 768), float32] */\n",
      "  %654 = _contrib_reverse_reshape(%653, newshape=[-1, 0], reverse=True) /* ty=Tensor[(256, 768), float32] */\n",
      "  %655 = nn.dense(%654, %hybridbertencoder2_transformer9_multiheadattentioncell0_query_weight, units=768) /* ty=Tensor[(256, 768), float32] */\n",
      "  %656 = nn.bias_add(%655, %hybridbertencoder2_transformer9_multiheadattentioncell0_query_bias, axis=-1) /* ty=Tensor[(256, 768), float32] */\n",
      "  %657 = reshape(%656, newshape=[1, 256, 768]) /* ty=Tensor[(1, 256, 768), float32] */\n",
      "  %658 = reshape(%657, newshape=[0, 0, 12, -1]) /* ty=Tensor[(1, 256, 12, 64), float32] */\n",
      "  %659 = transpose(%658, axes=[0, 2, 1, 3]) /* ty=Tensor[(1, 12, 256, 64), float32] */\n",
      "  %660 = _contrib_reverse_reshape(%659, newshape=[-1, 0, 0], reverse=True) /* ty=Tensor[(12, 256, 64), float32] */\n",
      "  %661 = shape_of(%660, dtype=\"int32\") /* ty=Tensor[(3,), int32] */\n",
      "  %662 = take(%661, 2 /* ty=int32 */) /* ty=int32 */\n",
      "  %663 = cast(%662, dtype=\"float32\") /* ty=float32 */\n",
      "  %664 = sqrt(%663) /* ty=float32 */\n",
      "  %665 = divide(%660, %664) /* ty=Tensor[(12, 256, 64), float32] */\n",
      "  %666 = _contrib_reverse_reshape(%653, newshape=[-1, 0], reverse=True) /* ty=Tensor[(256, 768), float32] */\n",
      "  %667 = nn.dense(%666, %hybridbertencoder2_transformer9_multiheadattentioncell0_key_weight, units=768) /* ty=Tensor[(256, 768), float32] */\n",
      "  %668 = nn.bias_add(%667, %hybridbertencoder2_transformer9_multiheadattentioncell0_key_bias, axis=-1) /* ty=Tensor[(256, 768), float32] */\n",
      "  %669 = reshape(%668, newshape=[1, 256, 768]) /* ty=Tensor[(1, 256, 768), float32] */\n",
      "  %670 = reshape(%669, newshape=[0, 0, 12, -1]) /* ty=Tensor[(1, 256, 12, 64), float32] */\n",
      "  %671 = transpose(%670, axes=[0, 2, 1, 3]) /* ty=Tensor[(1, 12, 256, 64), float32] */\n",
      "  %672 = _contrib_reverse_reshape(%671, newshape=[-1, 0, 0], reverse=True) /* ty=Tensor[(12, 256, 64), float32] */\n",
      "  %673 = nn.batch_matmul(%665, %672) /* ty=Tensor[(12, 256, 256), float32] */\n",
      "  %674 = ones_like(%673) /* ty=Tensor[(12, 256, 256), float32] */\n",
      "  %675 = multiply(%674, -1e+18f /* ty=float32 */) /* ty=Tensor[(12, 256, 256), float32] */\n",
      "  %676 = where(%15, %673, %675) /* ty=Tensor[(12, 256, 256), float32] */\n",
      "  %677 = nn.softmax(%676) /* ty=Tensor[(12, 256, 256), float32] */\n",
      "  %678 = multiply(%677, %15) /* ty=Tensor[(12, 256, 256), float32] */\n",
      "  %679 = nn.dropout(%678, rate=0.1) /* ty=(Tensor[(12, 256, 256), float32], Tensor[(12, 256, 256), float32]) */\n",
      "  %680 = %679.0\n",
      "  %681 = _contrib_reverse_reshape(%680, newshape=[-1, 12, 0, 0], reverse=True) /* ty=Tensor[(1, 12, 256, 256), float32] */\n",
      "  %682 = _contrib_reverse_reshape(%681, newshape=[-1, 0, 0], reverse=True) /* ty=Tensor[(12, 256, 256), float32] */\n",
      "  %683 = _contrib_reverse_reshape(%653, newshape=[-1, 0], reverse=True) /* ty=Tensor[(256, 768), float32] */\n",
      "  %684 = nn.dense(%683, %hybridbertencoder2_transformer9_multiheadattentioncell0_value_weight, units=768) /* ty=Tensor[(256, 768), float32] */\n",
      "  %685 = nn.bias_add(%684, %hybridbertencoder2_transformer9_multiheadattentioncell0_value_bias, axis=-1) /* ty=Tensor[(256, 768), float32] */\n",
      "  %686 = reshape(%685, newshape=[1, 256, 768]) /* ty=Tensor[(1, 256, 768), float32] */\n",
      "  %687 = reshape(%686, newshape=[0, 0, 12, -1]) /* ty=Tensor[(1, 256, 12, 64), float32] */\n",
      "  %688 = transpose(%687, axes=[0, 2, 1, 3]) /* ty=Tensor[(1, 12, 256, 64), float32] */\n",
      "  %689 = _contrib_reverse_reshape(%688, newshape=[-1, 0, 0], reverse=True) /* ty=Tensor[(12, 256, 64), float32] */\n",
      "  %690 = transpose(%689, axes=[0, 2, 1]) /* ty=Tensor[(12, 64, 256), float32] */\n",
      "  %691 = nn.batch_matmul(%682, %690) /* ty=Tensor[(12, 256, 64), float32] */\n",
      "  %692 = _contrib_reverse_reshape(%691, newshape=[-1, 12, 0, 0], reverse=True) /* ty=Tensor[(1, 12, 256, 64), float32] */\n",
      "  %693 = transpose(%692, axes=[0, 2, 1, 3]) /* ty=Tensor[(1, 256, 12, 64), float32] */\n",
      "  %694 = reshape(%693, newshape=[0, 0, -1]) /* ty=Tensor[(1, 256, 768), float32] */\n",
      "  %695 = _contrib_reverse_reshape(%694, newshape=[-1, 0], reverse=True) /* ty=Tensor[(256, 768), float32] */\n",
      "  %696 = nn.dense(%695, %hybridbertencoder2_transformer9_proj_weight, units=768) /* ty=Tensor[(256, 768), float32] */\n",
      "  %697 = nn.bias_add(%696, %hybridbertencoder2_transformer9_proj_bias, axis=-1) /* ty=Tensor[(256, 768), float32] */\n",
      "  %698 = reshape(%697, newshape=[1, 256, 768]) /* ty=Tensor[(1, 256, 768), float32] */\n",
      "  %699 = nn.dropout(%698, rate=0.1) /* ty=(Tensor[(1, 256, 768), float32], Tensor[(1, 256, 768), float32]) */\n",
      "  %700 = %699.0\n",
      "  %701 = add(%700, %653) /* ty=Tensor[(1, 256, 768), float32] */\n",
      "  %702 = nn.layer_norm(%701, %hybridbertencoder2_transformer9_bertlayernorm0_gamma, %hybridbertencoder2_transformer9_bertlayernorm0_beta, epsilon=1e-12) /* ty=Tensor[(1, 256, 768), float32] */\n",
      "  %703 = _contrib_reverse_reshape(%702, newshape=[-1, 0], reverse=True) /* ty=Tensor[(256, 768), float32] */\n",
      "  %704 = nn.dense(%703, %hybridbertencoder2_transformer9_bertpositionwiseffn0_ffn_1_weight, units=3072) /* ty=Tensor[(256, 3072), float32] */\n",
      "  %705 = nn.bias_add(%704, %hybridbertencoder2_transformer9_bertpositionwiseffn0_ffn_1_bias, axis=-1) /* ty=Tensor[(256, 3072), float32] */\n",
      "  %706 = reshape(%705, newshape=[1, 256, 3072]) /* ty=Tensor[(1, 256, 3072), float32] */\n",
      "  %707 = multiply(%706, 0.5f /* ty=float32 */) /* ty=Tensor[(1, 256, 3072), float32] */\n",
      "  %708 = divide(%706, 1.41421f /* ty=float32 */) /* ty=Tensor[(1, 256, 3072), float32] */\n",
      "  %709 = erf(%708) /* ty=Tensor[(1, 256, 3072), float32] */\n",
      "  %710 = add(%709, 1f /* ty=float32 */) /* ty=Tensor[(1, 256, 3072), float32] */\n",
      "  %711 = multiply(%707, %710) /* ty=Tensor[(1, 256, 3072), float32] */\n",
      "  %712 = _contrib_reverse_reshape(%711, newshape=[-1, 0], reverse=True) /* ty=Tensor[(256, 3072), float32] */\n",
      "  %713 = nn.dense(%712, %hybridbertencoder2_transformer9_bertpositionwiseffn0_ffn_2_weight, units=768) /* ty=Tensor[(256, 768), float32] */\n",
      "  %714 = nn.bias_add(%713, %hybridbertencoder2_transformer9_bertpositionwiseffn0_ffn_2_bias, axis=-1) /* ty=Tensor[(256, 768), float32] */\n",
      "  %715 = reshape(%714, newshape=[1, 256, 768]) /* ty=Tensor[(1, 256, 768), float32] */\n",
      "  %716 = nn.dropout(%715, rate=0.1) /* ty=(Tensor[(1, 256, 768), float32], Tensor[(1, 256, 768), float32]) */\n",
      "  %717 = %716.0\n",
      "  %718 = add(%717, %702) /* ty=Tensor[(1, 256, 768), float32] */\n",
      "  %719 = nn.layer_norm(%718, %hybridbertencoder2_transformer9_bertpositionwiseffn0_bertlayernorm0_gamma, %hybridbertencoder2_transformer9_bertpositionwiseffn0_bertlayernorm0_beta, epsilon=1e-12) /* ty=Tensor[(1, 256, 768), float32] */\n",
      "  %720 = _contrib_reverse_reshape(%719, newshape=[-1, 0], reverse=True) /* ty=Tensor[(256, 768), float32] */\n",
      "  %721 = nn.dense(%720, %hybridbertencoder2_transformer10_multiheadattentioncell0_query_weight, units=768) /* ty=Tensor[(256, 768), float32] */\n",
      "  %722 = nn.bias_add(%721, %hybridbertencoder2_transformer10_multiheadattentioncell0_query_bias, axis=-1) /* ty=Tensor[(256, 768), float32] */\n",
      "  %723 = reshape(%722, newshape=[1, 256, 768]) /* ty=Tensor[(1, 256, 768), float32] */\n",
      "  %724 = reshape(%723, newshape=[0, 0, 12, -1]) /* ty=Tensor[(1, 256, 12, 64), float32] */\n",
      "  %725 = transpose(%724, axes=[0, 2, 1, 3]) /* ty=Tensor[(1, 12, 256, 64), float32] */\n",
      "  %726 = _contrib_reverse_reshape(%725, newshape=[-1, 0, 0], reverse=True) /* ty=Tensor[(12, 256, 64), float32] */\n",
      "  %727 = shape_of(%726, dtype=\"int32\") /* ty=Tensor[(3,), int32] */\n",
      "  %728 = take(%727, 2 /* ty=int32 */) /* ty=int32 */\n",
      "  %729 = cast(%728, dtype=\"float32\") /* ty=float32 */\n",
      "  %730 = sqrt(%729) /* ty=float32 */\n",
      "  %731 = divide(%726, %730) /* ty=Tensor[(12, 256, 64), float32] */\n",
      "  %732 = _contrib_reverse_reshape(%719, newshape=[-1, 0], reverse=True) /* ty=Tensor[(256, 768), float32] */\n",
      "  %733 = nn.dense(%732, %hybridbertencoder2_transformer10_multiheadattentioncell0_key_weight, units=768) /* ty=Tensor[(256, 768), float32] */\n",
      "  %734 = nn.bias_add(%733, %hybridbertencoder2_transformer10_multiheadattentioncell0_key_bias, axis=-1) /* ty=Tensor[(256, 768), float32] */\n",
      "  %735 = reshape(%734, newshape=[1, 256, 768]) /* ty=Tensor[(1, 256, 768), float32] */\n",
      "  %736 = reshape(%735, newshape=[0, 0, 12, -1]) /* ty=Tensor[(1, 256, 12, 64), float32] */\n",
      "  %737 = transpose(%736, axes=[0, 2, 1, 3]) /* ty=Tensor[(1, 12, 256, 64), float32] */\n",
      "  %738 = _contrib_reverse_reshape(%737, newshape=[-1, 0, 0], reverse=True) /* ty=Tensor[(12, 256, 64), float32] */\n",
      "  %739 = nn.batch_matmul(%731, %738) /* ty=Tensor[(12, 256, 256), float32] */\n",
      "  %740 = ones_like(%739) /* ty=Tensor[(12, 256, 256), float32] */\n",
      "  %741 = multiply(%740, -1e+18f /* ty=float32 */) /* ty=Tensor[(12, 256, 256), float32] */\n",
      "  %742 = where(%12, %739, %741) /* ty=Tensor[(12, 256, 256), float32] */\n",
      "  %743 = nn.softmax(%742) /* ty=Tensor[(12, 256, 256), float32] */\n",
      "  %744 = multiply(%743, %12) /* ty=Tensor[(12, 256, 256), float32] */\n",
      "  %745 = nn.dropout(%744, rate=0.1) /* ty=(Tensor[(12, 256, 256), float32], Tensor[(12, 256, 256), float32]) */\n",
      "  %746 = %745.0\n",
      "  %747 = _contrib_reverse_reshape(%746, newshape=[-1, 12, 0, 0], reverse=True) /* ty=Tensor[(1, 12, 256, 256), float32] */\n",
      "  %748 = _contrib_reverse_reshape(%747, newshape=[-1, 0, 0], reverse=True) /* ty=Tensor[(12, 256, 256), float32] */\n",
      "  %749 = _contrib_reverse_reshape(%719, newshape=[-1, 0], reverse=True) /* ty=Tensor[(256, 768), float32] */\n",
      "  %750 = nn.dense(%749, %hybridbertencoder2_transformer10_multiheadattentioncell0_value_weight, units=768) /* ty=Tensor[(256, 768), float32] */\n",
      "  %751 = nn.bias_add(%750, %hybridbertencoder2_transformer10_multiheadattentioncell0_value_bias, axis=-1) /* ty=Tensor[(256, 768), float32] */\n",
      "  %752 = reshape(%751, newshape=[1, 256, 768]) /* ty=Tensor[(1, 256, 768), float32] */\n",
      "  %753 = reshape(%752, newshape=[0, 0, 12, -1]) /* ty=Tensor[(1, 256, 12, 64), float32] */\n",
      "  %754 = transpose(%753, axes=[0, 2, 1, 3]) /* ty=Tensor[(1, 12, 256, 64), float32] */\n",
      "  %755 = _contrib_reverse_reshape(%754, newshape=[-1, 0, 0], reverse=True) /* ty=Tensor[(12, 256, 64), float32] */\n",
      "  %756 = transpose(%755, axes=[0, 2, 1]) /* ty=Tensor[(12, 64, 256), float32] */\n",
      "  %757 = nn.batch_matmul(%748, %756) /* ty=Tensor[(12, 256, 64), float32] */\n",
      "  %758 = _contrib_reverse_reshape(%757, newshape=[-1, 12, 0, 0], reverse=True) /* ty=Tensor[(1, 12, 256, 64), float32] */\n",
      "  %759 = transpose(%758, axes=[0, 2, 1, 3]) /* ty=Tensor[(1, 256, 12, 64), float32] */\n",
      "  %760 = reshape(%759, newshape=[0, 0, -1]) /* ty=Tensor[(1, 256, 768), float32] */\n",
      "  %761 = _contrib_reverse_reshape(%760, newshape=[-1, 0], reverse=True) /* ty=Tensor[(256, 768), float32] */\n",
      "  %762 = nn.dense(%761, %hybridbertencoder2_transformer10_proj_weight, units=768) /* ty=Tensor[(256, 768), float32] */\n",
      "  %763 = nn.bias_add(%762, %hybridbertencoder2_transformer10_proj_bias, axis=-1) /* ty=Tensor[(256, 768), float32] */\n",
      "  %764 = reshape(%763, newshape=[1, 256, 768]) /* ty=Tensor[(1, 256, 768), float32] */\n",
      "  %765 = nn.dropout(%764, rate=0.1) /* ty=(Tensor[(1, 256, 768), float32], Tensor[(1, 256, 768), float32]) */\n",
      "  %766 = %765.0\n",
      "  %767 = add(%766, %719) /* ty=Tensor[(1, 256, 768), float32] */\n",
      "  %768 = nn.layer_norm(%767, %hybridbertencoder2_transformer10_bertlayernorm0_gamma, %hybridbertencoder2_transformer10_bertlayernorm0_beta, epsilon=1e-12) /* ty=Tensor[(1, 256, 768), float32] */\n",
      "  %769 = _contrib_reverse_reshape(%768, newshape=[-1, 0], reverse=True) /* ty=Tensor[(256, 768), float32] */\n",
      "  %770 = nn.dense(%769, %hybridbertencoder2_transformer10_bertpositionwiseffn0_ffn_1_weight, units=3072) /* ty=Tensor[(256, 3072), float32] */\n",
      "  %771 = nn.bias_add(%770, %hybridbertencoder2_transformer10_bertpositionwiseffn0_ffn_1_bias, axis=-1) /* ty=Tensor[(256, 3072), float32] */\n",
      "  %772 = reshape(%771, newshape=[1, 256, 3072]) /* ty=Tensor[(1, 256, 3072), float32] */\n",
      "  %773 = multiply(%772, 0.5f /* ty=float32 */) /* ty=Tensor[(1, 256, 3072), float32] */\n",
      "  %774 = divide(%772, 1.41421f /* ty=float32 */) /* ty=Tensor[(1, 256, 3072), float32] */\n",
      "  %775 = erf(%774) /* ty=Tensor[(1, 256, 3072), float32] */\n",
      "  %776 = add(%775, 1f /* ty=float32 */) /* ty=Tensor[(1, 256, 3072), float32] */\n",
      "  %777 = multiply(%773, %776) /* ty=Tensor[(1, 256, 3072), float32] */\n",
      "  %778 = _contrib_reverse_reshape(%777, newshape=[-1, 0], reverse=True) /* ty=Tensor[(256, 3072), float32] */\n",
      "  %779 = nn.dense(%778, %hybridbertencoder2_transformer10_bertpositionwiseffn0_ffn_2_weight, units=768) /* ty=Tensor[(256, 768), float32] */\n",
      "  %780 = nn.bias_add(%779, %hybridbertencoder2_transformer10_bertpositionwiseffn0_ffn_2_bias, axis=-1) /* ty=Tensor[(256, 768), float32] */\n",
      "  %781 = reshape(%780, newshape=[1, 256, 768]) /* ty=Tensor[(1, 256, 768), float32] */\n",
      "  %782 = nn.dropout(%781, rate=0.1) /* ty=(Tensor[(1, 256, 768), float32], Tensor[(1, 256, 768), float32]) */\n",
      "  %783 = %782.0\n",
      "  %784 = add(%783, %768) /* ty=Tensor[(1, 256, 768), float32] */\n",
      "  %785 = nn.layer_norm(%784, %hybridbertencoder2_transformer10_bertpositionwiseffn0_bertlayernorm0_gamma, %hybridbertencoder2_transformer10_bertpositionwiseffn0_bertlayernorm0_beta, epsilon=1e-12) /* ty=Tensor[(1, 256, 768), float32] */\n",
      "  %786 = _contrib_reverse_reshape(%785, newshape=[-1, 0], reverse=True) /* ty=Tensor[(256, 768), float32] */\n",
      "  %787 = nn.dense(%786, %hybridbertencoder2_transformer11_multiheadattentioncell0_query_weight, units=768) /* ty=Tensor[(256, 768), float32] */\n",
      "  %788 = nn.bias_add(%787, %hybridbertencoder2_transformer11_multiheadattentioncell0_query_bias, axis=-1) /* ty=Tensor[(256, 768), float32] */\n",
      "  %789 = reshape(%788, newshape=[1, 256, 768]) /* ty=Tensor[(1, 256, 768), float32] */\n",
      "  %790 = reshape(%789, newshape=[0, 0, 12, -1]) /* ty=Tensor[(1, 256, 12, 64), float32] */\n",
      "  %791 = transpose(%790, axes=[0, 2, 1, 3]) /* ty=Tensor[(1, 12, 256, 64), float32] */\n",
      "  %792 = _contrib_reverse_reshape(%791, newshape=[-1, 0, 0], reverse=True) /* ty=Tensor[(12, 256, 64), float32] */\n",
      "  %793 = shape_of(%792, dtype=\"int32\") /* ty=Tensor[(3,), int32] */\n",
      "  %794 = take(%793, 2 /* ty=int32 */) /* ty=int32 */\n",
      "  %795 = cast(%794, dtype=\"float32\") /* ty=float32 */\n",
      "  %796 = sqrt(%795) /* ty=float32 */\n",
      "  %797 = divide(%792, %796) /* ty=Tensor[(12, 256, 64), float32] */\n",
      "  %798 = _contrib_reverse_reshape(%785, newshape=[-1, 0], reverse=True) /* ty=Tensor[(256, 768), float32] */\n",
      "  %799 = nn.dense(%798, %hybridbertencoder2_transformer11_multiheadattentioncell0_key_weight, units=768) /* ty=Tensor[(256, 768), float32] */\n",
      "  %800 = nn.bias_add(%799, %hybridbertencoder2_transformer11_multiheadattentioncell0_key_bias, axis=-1) /* ty=Tensor[(256, 768), float32] */\n",
      "  %801 = reshape(%800, newshape=[1, 256, 768]) /* ty=Tensor[(1, 256, 768), float32] */\n",
      "  %802 = reshape(%801, newshape=[0, 0, 12, -1]) /* ty=Tensor[(1, 256, 12, 64), float32] */\n",
      "  %803 = transpose(%802, axes=[0, 2, 1, 3]) /* ty=Tensor[(1, 12, 256, 64), float32] */\n",
      "  %804 = _contrib_reverse_reshape(%803, newshape=[-1, 0, 0], reverse=True) /* ty=Tensor[(12, 256, 64), float32] */\n",
      "  %805 = nn.batch_matmul(%797, %804) /* ty=Tensor[(12, 256, 256), float32] */\n",
      "  %806 = ones_like(%805) /* ty=Tensor[(12, 256, 256), float32] */\n",
      "  %807 = multiply(%806, -1e+18f /* ty=float32 */) /* ty=Tensor[(12, 256, 256), float32] */\n",
      "  %808 = where(%9, %805, %807) /* ty=Tensor[(12, 256, 256), float32] */\n",
      "  %809 = nn.softmax(%808) /* ty=Tensor[(12, 256, 256), float32] */\n",
      "  %810 = multiply(%809, %9) /* ty=Tensor[(12, 256, 256), float32] */\n",
      "  %811 = nn.dropout(%810, rate=0.1) /* ty=(Tensor[(12, 256, 256), float32], Tensor[(12, 256, 256), float32]) */\n",
      "  %812 = %811.0\n",
      "  %813 = _contrib_reverse_reshape(%812, newshape=[-1, 12, 0, 0], reverse=True) /* ty=Tensor[(1, 12, 256, 256), float32] */\n",
      "  %814 = _contrib_reverse_reshape(%813, newshape=[-1, 0, 0], reverse=True) /* ty=Tensor[(12, 256, 256), float32] */\n",
      "  %815 = _contrib_reverse_reshape(%785, newshape=[-1, 0], reverse=True) /* ty=Tensor[(256, 768), float32] */\n",
      "  %816 = nn.dense(%815, %hybridbertencoder2_transformer11_multiheadattentioncell0_value_weight, units=768) /* ty=Tensor[(256, 768), float32] */\n",
      "  %817 = nn.bias_add(%816, %hybridbertencoder2_transformer11_multiheadattentioncell0_value_bias, axis=-1) /* ty=Tensor[(256, 768), float32] */\n",
      "  %818 = reshape(%817, newshape=[1, 256, 768]) /* ty=Tensor[(1, 256, 768), float32] */\n",
      "  %819 = reshape(%818, newshape=[0, 0, 12, -1]) /* ty=Tensor[(1, 256, 12, 64), float32] */\n",
      "  %820 = transpose(%819, axes=[0, 2, 1, 3]) /* ty=Tensor[(1, 12, 256, 64), float32] */\n",
      "  %821 = _contrib_reverse_reshape(%820, newshape=[-1, 0, 0], reverse=True) /* ty=Tensor[(12, 256, 64), float32] */\n",
      "  %822 = transpose(%821, axes=[0, 2, 1]) /* ty=Tensor[(12, 64, 256), float32] */\n",
      "  %823 = nn.batch_matmul(%814, %822) /* ty=Tensor[(12, 256, 64), float32] */\n",
      "  %824 = _contrib_reverse_reshape(%823, newshape=[-1, 12, 0, 0], reverse=True) /* ty=Tensor[(1, 12, 256, 64), float32] */\n",
      "  %825 = transpose(%824, axes=[0, 2, 1, 3]) /* ty=Tensor[(1, 256, 12, 64), float32] */\n",
      "  %826 = reshape(%825, newshape=[0, 0, -1]) /* ty=Tensor[(1, 256, 768), float32] */\n",
      "  %827 = _contrib_reverse_reshape(%826, newshape=[-1, 0], reverse=True) /* ty=Tensor[(256, 768), float32] */\n",
      "  %828 = nn.dense(%827, %hybridbertencoder2_transformer11_proj_weight, units=768) /* ty=Tensor[(256, 768), float32] */\n",
      "  %829 = nn.bias_add(%828, %hybridbertencoder2_transformer11_proj_bias, axis=-1) /* ty=Tensor[(256, 768), float32] */\n",
      "  %830 = reshape(%829, newshape=[1, 256, 768]) /* ty=Tensor[(1, 256, 768), float32] */\n",
      "  %831 = nn.dropout(%830, rate=0.1) /* ty=(Tensor[(1, 256, 768), float32], Tensor[(1, 256, 768), float32]) */\n",
      "  %832 = %831.0\n",
      "  %833 = add(%832, %785) /* ty=Tensor[(1, 256, 768), float32] */\n",
      "  %834 = nn.layer_norm(%833, %hybridbertencoder2_transformer11_bertlayernorm0_gamma, %hybridbertencoder2_transformer11_bertlayernorm0_beta, epsilon=1e-12) /* ty=Tensor[(1, 256, 768), float32] */\n",
      "  %835 = _contrib_reverse_reshape(%834, newshape=[-1, 0], reverse=True) /* ty=Tensor[(256, 768), float32] */\n",
      "  %836 = nn.dense(%835, %hybridbertencoder2_transformer11_bertpositionwiseffn0_ffn_1_weight, units=3072) /* ty=Tensor[(256, 3072), float32] */\n",
      "  %837 = nn.bias_add(%836, %hybridbertencoder2_transformer11_bertpositionwiseffn0_ffn_1_bias, axis=-1) /* ty=Tensor[(256, 3072), float32] */\n",
      "  %838 = reshape(%837, newshape=[1, 256, 3072]) /* ty=Tensor[(1, 256, 3072), float32] */\n",
      "  %839 = multiply(%838, 0.5f /* ty=float32 */) /* ty=Tensor[(1, 256, 3072), float32] */\n",
      "  %840 = divide(%838, 1.41421f /* ty=float32 */) /* ty=Tensor[(1, 256, 3072), float32] */\n",
      "  %841 = erf(%840) /* ty=Tensor[(1, 256, 3072), float32] */\n",
      "  %842 = add(%841, 1f /* ty=float32 */) /* ty=Tensor[(1, 256, 3072), float32] */\n",
      "  %843 = multiply(%839, %842) /* ty=Tensor[(1, 256, 3072), float32] */\n",
      "  %844 = _contrib_reverse_reshape(%843, newshape=[-1, 0], reverse=True) /* ty=Tensor[(256, 3072), float32] */\n",
      "  %845 = nn.dense(%844, %hybridbertencoder2_transformer11_bertpositionwiseffn0_ffn_2_weight, units=768) /* ty=Tensor[(256, 768), float32] */\n",
      "  %846 = nn.bias_add(%845, %hybridbertencoder2_transformer11_bertpositionwiseffn0_ffn_2_bias, axis=-1) /* ty=Tensor[(256, 768), float32] */\n",
      "  %847 = reshape(%846, newshape=[1, 256, 768]) /* ty=Tensor[(1, 256, 768), float32] */\n",
      "  %848 = nn.dropout(%847, rate=0.1) /* ty=(Tensor[(1, 256, 768), float32], Tensor[(1, 256, 768), float32]) */\n",
      "  %849 = %848.0\n",
      "  %850 = add(%849, %834) /* ty=Tensor[(1, 256, 768), float32] */\n",
      "  %851 = nn.layer_norm(%850, %hybridbertencoder2_transformer11_bertpositionwiseffn0_bertlayernorm0_gamma, %hybridbertencoder2_transformer11_bertpositionwiseffn0_bertlayernorm0_beta, epsilon=1e-12) /* ty=Tensor[(1, 256, 768), float32] */\n",
      "  %852 = sequence_mask(%851, %data2, axis=1) /* ty=Tensor[(1, 256, 768), float32] */\n",
      "  %853 = _contrib_reverse_reshape(%852, newshape=[-1, 0], reverse=True) /* ty=Tensor[(256, 768), float32] */\n",
      "  %854 = nn.dense(%853, %hybridbertforqa2_dense0_weight, units=2) /* ty=Tensor[(256, 2), float32] */\n",
      "  %855 = nn.bias_add(%854, %hybridbertforqa2_dense0_bias, axis=-1) /* ty=Tensor[(256, 2), float32] */\n",
      "  reshape(%855, newshape=[1, 256, 2]) /* ty=Tensor[(1, 256, 2), float32] */\n",
      "}\n",
      "\n",
      "// meta data omitted. you can use show_meta_data=True to include meta data\n"
     ]
    }
   ],
   "source": [
    "shape_dict = {\n",
    "    'data0': (1, seq_length),\n",
    "    'data1': (1, seq_length),\n",
    "    'data2': (1,)\n",
    "}\n",
    "mod, params = relay.frontend.from_mxnet(net, shape_dict)\n",
    "print(mod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-14T01:45:32.575976Z",
     "start_time": "2019-06-14T01:45:32.223336Z"
    }
   },
   "outputs": [],
   "source": [
    "all_results = collections.defaultdict(list)\n",
    "\n",
    "total_num = 0\n",
    "for data in dev_dataloader:\n",
    "    example_ids, inputs, token_types, valid_length, _, _ = data\n",
    "    total_num += len(inputs)\n",
    "    batch_size = inputs.shape[0]\n",
    "    pred_start, pred_end = net(inputs.astype('float32').as_in_context(ctx),\n",
    "                               token_types.astype('float32').as_in_context(ctx),\n",
    "                               valid_length.astype('float32').as_in_context(ctx))\n",
    "\n",
    "    example_ids = example_ids.asnumpy().tolist()\n",
    "    pred_start = pred_start.reshape(batch_size, -1).asnumpy()\n",
    "    pred_end = pred_end.reshape(batch_size, -1).asnumpy()\n",
    "    \n",
    "    for example_id, start, end in zip(example_ids, pred_start, pred_end):\n",
    "        all_results[example_id].append(PredResult(start=start, end=end))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-14T01:45:32.623482Z",
     "start_time": "2019-06-14T01:45:32.578002Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Context: Super Bowl 50 was an American football game to determine the champion of the National Football League (NFL) for the 2015 season. The American Football Conference (AFC) champion Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers 24–10 to earn their third Super Bowl title. The game was played on February 7, 2016, at Levi's Stadium in the San Francisco Bay Area at Santa Clara, California. As this was the 50th Super Bowl, the league emphasized the \"golden anniversary\" with various gold-themed initiatives, as well as temporarily suspending the tradition of naming each Super Bowl game with Roman numerals (under which the game would have been known as \"Super Bowl L\"), so that the logo could prominently feature the Arabic numerals 50.\n",
      "\n",
      "Question: which nfl team represented the afc at super bowl 50 ?\n",
      "\n",
      "Top predictions: \n",
      "99.36% \t Denver Broncos\n",
      "0.23% \t The American Football Conference (AFC) champion Denver Broncos\n",
      "0.20% \t Broncos\n",
      "\n",
      "\n",
      "Context: Super Bowl 50 was an American football game to determine the champion of the National Football League (NFL) for the 2015 season. The American Football Conference (AFC) champion Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers 24–10 to earn their third Super Bowl title. The game was played on February 7, 2016, at Levi's Stadium in the San Francisco Bay Area at Santa Clara, California. As this was the 50th Super Bowl, the league emphasized the \"golden anniversary\" with various gold-themed initiatives, as well as temporarily suspending the tradition of naming each Super Bowl game with Roman numerals (under which the game would have been known as \"Super Bowl L\"), so that the logo could prominently feature the Arabic numerals 50.\n",
      "\n",
      "Question: which nfl team represented the nfc at super bowl 50 ?\n",
      "\n",
      "Top predictions: \n",
      "66.65% \t Carolina Panthers\n",
      "24.30% \t Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers\n",
      "7.42% \t Denver Broncos\n",
      "\n",
      "\n",
      "Context: Super Bowl 50 was an American football game to determine the champion of the National Football League (NFL) for the 2015 season. The American Football Conference (AFC) champion Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers 24–10 to earn their third Super Bowl title. The game was played on February 7, 2016, at Levi's Stadium in the San Francisco Bay Area at Santa Clara, California. As this was the 50th Super Bowl, the league emphasized the \"golden anniversary\" with various gold-themed initiatives, as well as temporarily suspending the tradition of naming each Super Bowl game with Roman numerals (under which the game would have been known as \"Super Bowl L\"), so that the logo could prominently feature the Arabic numerals 50.\n",
      "\n",
      "Question: where did super bowl 50 take place ?\n",
      "\n",
      "Top predictions: \n",
      "25.86% \t Levi's Stadium in the San Francisco Bay Area at Santa Clara, California\n",
      "23.11% \t Levi's Stadium\n",
      "17.88% \t San Francisco Bay Area at Santa Clara, California\n",
      "\n"
     ]
    }
   ],
   "source": [
    "qa_utils.predict(dataset, all_results, vocab)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
